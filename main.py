# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.3 FINAL] FAST Google News RSS -> Slack Digest (1-day + Nexon precision)
# - Python 3.9 í˜¸í™˜
# - ëª©í‘œ: ê°€ë³ê³ (ìˆ˜ì‹­ ì´ˆ), ì¡ìŒ ì ê³ , ë„¥ìŠ¨ ì„¹ì…˜ ì •í™•ë„ ë†’ê²Œ
#
# ì¼ë°˜ íŠ¸ë™:
#   (ê²Œì„ ì»¨í…ìŠ¤íŠ¸) + í‚¤ì›Œë“œ + (site OR ...) + when:1d
# ë„¥ìŠ¨ íŠ¸ë™(ì •ë°€ë„ ìš°ì„ ):
#   (ë„¥ìŠ¨ í‘œí˜„ì‹) + í‚¤ì›Œë“œ + (site OR ...) + when:1d
#   + ë¡œì»¬ ê²€ì¦(ì œëª©/ìš”ì•½ì— ë„¥ìŠ¨ ë¬¸ìì—´ ì‹¤ì œ í¬í•¨) í•„ìˆ˜
#   + ì¤‘ìš”ë„ ì ìˆ˜ë¡œ Top 5ë§Œ ë…¸ì¶œ
#
# NOTE: ë³¸ë¬¸ í¬ë¡¤ë§/ë¦¬ì¡¸ë¸Œ/HTML íŒŒì‹± ì—†ìŒ(ë¬´ê²ì§€ ì•Šê²Œ)
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote

import requests
import feedparser

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

# âœ… í•˜ë£¨ì¹˜ë§Œ
SEARCH_DAYS = 1

# ì„±ëŠ¥/ì•ˆì • ë°¸ëŸ°ìŠ¤
KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18
MAX_ENTRIES_PER_FEED = 30
MAX_ENTRIES_PER_NEXON_FEED = 20  # ë„¥ìŠ¨ì€ ì ê²Œ(ì •í™•ë„/ì†ë„)

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.3; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.12)

# Slack/í¬ë§· ì œí•œ
SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 15

# ==========================
# ì»¨í…ìŠ¤íŠ¸/í•„í„°
# ==========================
# ì¼ë°˜ ë‰´ìŠ¤ ë…¸ì´ì¦ˆ ì–µì œìš© "ê²Œì„ ì»¨í…ìŠ¤íŠ¸"
GAME_CONTEXT_OR = [
    "ê²Œì„", "ê²Œì´ë°", "ê²Œì„ì—…ê³„", "ê²Œì„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œì„", "PCê²Œì„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports"
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

# ì¢…í•© IT/ê²½ì œ ë§¤ì²´ëŠ” ì¶”ê°€ë¡œ ë¹¡ì„¸ê²Œ(ì¼ë°˜ íŠ¸ë™ë§Œ ì ìš©)
STRICT_SITES = {"zdnet.co.kr", "ddaily.co.kr"}

GAME_HINTS = [
    "ê²Œì„", "ê²Œì´ë°", "ì‹ ì‘", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ", "ìŠ¤íŒ€", "ì½˜ì†”", "ëª¨ë°”ì¼", "PC",
    "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox", "RPG", "MMORPG", "FPS", "MOBA",
    "eìŠ¤í¬ì¸ ", "esports",
    "ë„¥ìŠ¨", "ì—”ì”¨", "í¬ë˜í”„í†¤", "ë„·ë§ˆë¸”", "ì¹´ì¹´ì˜¤ê²Œì„", "ìŠ¤ë§ˆì¼ê²Œì´íŠ¸", "í„ì–´ë¹„ìŠ¤",
]

# ë„¥ìŠ¨ â€œì‹¤ì¡´ ê²€ì¦â€ ìš©ì–´(ì œëª©/ìš”ì•½ì— ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨)
NEXON_TERMS = [
    "ë„¥ìŠ¨", "nexon",
    "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œì„ì¦ˆ", "ë„¥ìŠ¨ ë„¤íŠ¸ì›ìŠ¤", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤",
    "ë„¤ì˜¤í”Œ", "ë„¥ìŠ¨GT", "ë„¥ìŠ¨ì§€í‹°",
]

# ë„¥ìŠ¨ ì¤‘ìš”ë„ ì ìˆ˜(ë¬¸ìì—´ í¬í•¨ ê¸°ë°˜, ë§¤ìš° ê°€ë²¼ì›€)
NEXON_IMPORTANCE = [
    ("M&A", 5), ("ì¸ìˆ˜", 5), ("í•©ë³‘", 5),
    ("íˆ¬ì", 4), ("ì§€ë¶„", 4),
    ("ì†Œì†¡", 5), ("ê·œì œ", 4),
    ("ë§¤ì¶œ", 4), ("ì‹¤ì ", 4), ("ì˜ì—…ì´ìµ", 4), ("ìˆœì´ìµ", 4),
    ("ì¶œì‹œ", 3), ("ì—…ë°ì´íŠ¸", 3),
    ("CBT", 2), ("OBT", 2),
    ("ë¦¬ìŠ¤í¬", 3), ("ì•…ì¬", 3), ("í˜¸ì¬", 3),
]

# ==========================
# ìœ í‹¸
# ==========================
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess(entry) -> str:
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _has_any_hint(text: str, hints: List[str]) -> bool:
    blob = (text or "").lower()
    return any(h.lower() in blob for h in hints)

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

def nexon_score(article: Dict) -> int:
    blob = f"{article.get('title','')} {article.get('snippet','')}".lower()
    score = 0
    for kw, w in NEXON_IMPORTANCE:
        if kw.lower() in blob:
            score += w
    # ë„¥ìŠ¨ì´ ì‹¤ì œë¡œ ë“¤ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ê°€ì‚°
    if contains_nexon(article.get("title", ""), article.get("snippet", "")):
        score += 2
    return score

# ==========================
# ì¿¼ë¦¬ ë¹Œë”
# ==========================
def build_query_general(keyword: str, sites: List[str], days: int) -> str:
    if sites:
        return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} when:{days}d"
    return f"{GAME_CONTEXT_QUERY} {keyword} when:{days}d"

def build_query_nexon(keyword: str, sites: List[str], days: int) -> str:
    # ë„¥ìŠ¨ì€ êµì§‘í•©(ë„¥ìŠ¨ AND í‚¤ì›Œë“œ)ë§Œ
    nexon_expr = '("ë„¥ìŠ¨" OR Nexon OR "ë„¥ìŠ¨ê²Œì„ì¦ˆ" OR ë„¤ì˜¤í”Œ)'
    if sites:
        return f'{nexon_expr} {keyword} {_site_or_query(sites)} when:{days}d'
    return f'{nexon_expr} {keyword} when:{days}d'

# ==========================
# RSS ìˆ˜ì§‘
# ==========================
def fetch_general(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "strict_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_general(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                # ì¼ë°˜ íŠ¸ë™: zdnet/ddailyëŠ” ê²Œì„ íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì œê±°
                # (linkê°€ news.google ì¤‘ê°„ë§í¬ì—¬ë„ title/snippetë¡œ ì¶©ë¶„íˆ ê±¸ëŸ¬ì§)
                if any(s in link for s in STRICT_SITES) or any(s in title for s in ("ì§€ë””ë„·", "ë””ì§€í„¸ë°ì¼ë¦¬")):
                    if not _has_any_hint(f"{title} {snippet}", GAME_HINTS):
                        stats["strict_filtered_out"] += 1
                        continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "general",
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (general kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

def fetch_nexon(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "nexon_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_nexon(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_NEXON_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                # âœ… ìµœì¢… ê²€ì¦: ì œëª©/ìš”ì•½ì— ë„¥ìŠ¨ì´ ì‹¤ì œë¡œ ìˆì–´ì•¼ë§Œ ë„¥ìŠ¨ ì„¹ì…˜ì— í¬í•¨
                if not contains_nexon(title, snippet):
                    stats["nexon_filtered_out"] += 1
                    continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "nexon",
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (nexon kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

# ==========================
# Slack ë©”ì‹œì§€
# ==========================
def build_messages(general: List[Dict], nexon: List[Dict],
                   stats_g: Dict[str, int], stats_n: Dict[str, int], days: int) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {days}ì¼)\n"
    header += f"- general: feeds={stats_g.get('feeds_called',0)}, entries={stats_g.get('entries_seen',0)}, added={stats_g.get('added',0)}, strict_drop={stats_g.get('strict_filtered_out',0)}\n"
    header += f"- nexon: feeds={stats_n.get('feeds_called',0)}, entries={stats_n.get('entries_seen',0)}, added={stats_n.get('added',0)}, nexon_drop={stats_n.get('nexon_filtered_out',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not general:
        body += "- ì˜¤ëŠ˜ ê¸°ì¤€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in general[:70]:
            body += fmt(a)

    # ë„¥ìŠ¨ì€ ì¤‘ìš”ë„ ì ìˆ˜ë¡œ Top 5
    if nexon:
        scored = sorted(nexon, key=lambda x: (nexon_score(x), x["published_dt"] or datetime.min), reverse=True)
    else:
        scored = []

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ (Top 5)\n"
    if not scored:
        body += "- ë„¥ìŠ¨ ê´€ë ¨ ë‰´ìŠ¤(í‚¤ì›Œë“œ êµì§‘í•© + ì œëª©/ìš”ì•½ ê²€ì¦)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
    else:
        for a in scored[:5]:
            body += fmt(a)

    full = header + body

    # Slack ê¸¸ì´ ë¶„í• 
    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

# ==========================
# Main
# ==========================
def main() -> None:
    # ì¼ë°˜ íŠ¸ë™: ìƒìœ„ í‚¤ì›Œë“œ ìš°ì„ , 0ì´ë©´ í™•ì¥
    general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY], TARGET_SITES, SEARCH_DAYS)
    if not general:
        general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, SEARCH_DAYS)

    # ë„¥ìŠ¨ íŠ¸ë™: "ë„¥ìŠ¨ AND í‚¤ì›Œë“œ" êµì§‘í•©ë§Œ (ì •ë°€ë„ ìš°ì„ )
    nexon, stats_n = fetch_nexon(PRIMARY_KEYWORDS, TARGET_SITES, SEARCH_DAYS)

    print(f"[INFO] general fetched: {len(general)}, stats: {stats_g}")
    print(f"[INFO] nexon fetched: {len(nexon)}, stats: {stats_n}")
    print("[INFO] preview general:")
    for i, a in enumerate(general[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    messages = build_messages(general, nexon, stats_g, stats_n, SEARCH_DAYS)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.15)

if __name__ == "__main__":
    main()
