# ------------------------------------------------------------------
# [ìˆ˜ì •ë³¸] googlesearch-python advanced + tbs(cdr)ë¡œ ì œëª©/ê¸°ê°„ ì •í™•ë„ ê°œì„  (2026-02-23)
# ------------------------------------------------------------------
import os
import json
import time
import random
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, urlunparse

# pip install googlesearch-python
from googlesearch import search

# --- (1) ì„¤ì • ë¶€ë¶„ ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")  # KeyError ë°©ì§€

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

SEARCH_DAYS = 14
MAX_RESULTS_PER_QUERY = 5

# --- (2) ìœ í‹¸ ---
def _normalize_url(raw_url: str) -> str:
    """
    Google ê²°ê³¼ì— ì¢…ì¢… ì„ì´ëŠ” ë¦¬ë‹¤ì´ë ‰íŠ¸/íŠ¸ë˜í‚¹/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°.
    - https://www.google.com/url?q=... í˜•íƒœë©´ q íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ URLë¡œ ì‚¬ìš©
    - fragment(#...) ì œê±°
    - í”í•œ UTM ì œê±°
    """
    if not raw_url:
        return raw_url

    # Google redirect URL ì²˜ë¦¬
    try:
        p = urlparse(raw_url)
        if p.netloc in ("www.google.com", "google.com") and p.path == "/url":
            q = parse_qs(p.query).get("q")
            if q and q[0]:
                raw_url = q[0]
    except Exception:
        pass

    try:
        p = urlparse(raw_url)
        qs = parse_qs(p.query)
        # UTM ì œê±°
        for k in list(qs.keys()):
            if k.lower().startswith("utm_"):
                qs.pop(k, None)

        # query ì¬ì¡°ë¦½
        new_query = "&".join(
            f"{k}={v[0]}" if len(v) == 1 else "&".join([f"{k}={x}" for x in v])
            for k, v in qs.items()
        )
        cleaned = urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, ""))  # fragment ì œê±°
        return cleaned
    except Exception:
        return raw_url

def _press_from_url(url: str) -> str:
    """ë„ë©”ì¸ì—ì„œ ì–¸ë¡ ì‚¬/ë§¤ì²´ ë¼ë²¨ ìƒì„±(ê°„ë‹¨ ë²„ì „)."""
    try:
        netloc = urlparse(url).netloc.lower()
        netloc = netloc.replace("www.", "")
        # ì˜ˆ: zdnet.co.kr -> zdnet
        base = netloc.split(".")[0]
        return base.upper() if base else "NEWS"
    except Exception:
        return "NEWS"

def _build_tbs_custom_range(start_dt: datetime, end_dt: datetime) -> str:
    """
    Google ê²€ìƒ‰ tbs ì»¤ìŠ¤í…€ ê¸°ê°„:
    tbs=cdr:1,cd_min:MM/DD/YYYY,cd_max:MM/DD/YYYY
    (googlesearch-pythonì´ tbsë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŒ) :contentReference[oaicite:2]{index=2}
    """
    cd_min = start_dt.strftime("%m/%d/%Y")
    cd_max = end_dt.strftime("%m/%d/%Y")
    return f"cdr:1,cd_min:{cd_min},cd_max:{cd_max}"

# --- (3) ë©”ì¸ ë¡œì§ ---
def find_news_by_google():
    now = datetime.now()
    start_dt = now - timedelta(days=SEARCH_DAYS)
    end_dt = now

    tbs = _build_tbs_custom_range(start_dt, end_dt)

    found_articles = {}  # normalized_url -> article

    for keyword in PRIMARY_KEYWORDS:
        for site in TARGET_SITES:
            query = f'"{keyword}" site:{site}'
            try:
                # advanced=Trueë©´ title/url/descriptionì„ ë°›ìŒ :contentReference[oaicite:3]{index=3}
                # tbsë¡œ ê¸°ê°„ ê°•ì œ :contentReference[oaicite:4]{index=4}
                results = search(
                    query,
                    lang="ko",
                    advanced=True,
                    tbs=tbs,
                    num=MAX_RESULTS_PER_QUERY,
                    stop=MAX_RESULTS_PER_QUERY,
                    pause=random.uniform(2.0, 3.5),
                )

                for r in results:
                    # r: SearchResult (title, url, description) :contentReference[oaicite:5]{index=5}
                    url = _normalize_url(getattr(r, "url", "") or "")
                    if not url:
                        continue

                    # ë„ë©”ì¸ í•„í„°(ì•ˆì „ë§)
                    if site not in url:
                        continue

                    title = (getattr(r, "title", "") or "").strip()
                    desc = (getattr(r, "description", "") or "").strip()

                    # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´(ê°€ë” ìˆìŒ) ë§ˆì§€ë§‰ fallbackìœ¼ë¡œ URL ì¡°ê° ì‚¬ìš©
                    if not title:
                        title = urlparse(url).path.strip("/").split("/")[-1].replace("-", " ").replace("_", " ")

                    if url not in found_articles:
                        found_articles[url] = {
                            "press": _press_from_url(url),
                            "title": title,
                            "link": url,
                            "desc": desc,
                        }

                # (ì„ íƒ) ì¿¼ë¦¬ ì‚¬ì´ ì•½ê°„ ì‰¬ì–´ì£¼ê¸°(ì°¨ë‹¨/429 ë°©ì§€)
                time.sleep(random.uniform(0.3, 0.8))

            except Exception as e:
                print(f"[WARN] êµ¬ê¸€ ê²€ìƒ‰ ì˜¤ë¥˜ (keyword={keyword}, site={site}): {e}")
                continue

    all_articles = list(found_articles.values())

    # ë„¥ìŠ¨ ê´€ë ¨: title/desc/url ëª¨ë‘ì—ì„œ íƒì§€ (ê¸°ì¡´ë³´ë‹¤ ì •í™•)
    def is_nexon(a):
        blob = f"{a.get('title','')} {a.get('desc','')} {a.get('link','')}".lower()
        return ("ë„¥ìŠ¨" in blob) or ("nexon" in blob)

    nexon_articles = [a for a in all_articles if is_nexon(a)]
    return all_articles, nexon_articles

def create_report_message():
    all_articles, nexon_articles = find_news_by_google()
    today_str = datetime.now().strftime("%Y-%m-%d")

    msg = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {SEARCH_DAYS}ì¼, Google ê²€ìƒ‰)\n\n"

    msg += "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not all_articles:
        msg += f"- ìµœê·¼ {SEARCH_DAYS}ì¼ê°„, ì§€ì •ëœ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
    else:
        for a in all_articles:
            # Slack ë§í¬ í¬ë§·: <url|text>
            msg += f"â–¶ *[{a['press']}]* <{a['link']}|{a['title']}>\n"
        msg += "\n"

    msg += "---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤\n"
    if not nexon_articles:
        msg += "- ìœ„ ê¸°ì‚¬ë“¤ ì¤‘, 'ë„¥ìŠ¨' ê´€ë ¨ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë‰´ìŠ¤ëŠ” ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon_articles:
            msg += f"â–¶ *[{a['press']}]* <{a['link']}|{a['title']}>\n"

    return msg

def send_to_slack(message: str):
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    headers = {"Content-Type": "application/json"}
    resp = requests.post(SLACK_WEBHOOK_URL, data=json.dumps(payload), headers=headers, timeout=15)
    resp.raise_for_status()

if __name__ == "__main__":
    report = create_report_message()
    send_to_slack(report)
