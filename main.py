# ------------------------------------------------------------------
# [ìš´ì˜ìš© ìµœì¢… v3] Google News RSS + ì›ë³¸ ë§í¬ ì¶”ì¶œ(HTML íŒŒì‹±) + ë³¸ë¬¸ ìš”ì•½ + Slack ì „ì†¡
# - Python 3.9 í˜¸í™˜
# - 0ê±´ ë°©ì§€: (1) redirect resolve (2) news.google HTMLì—ì„œ ì›ë³¸ ë§í¬ ì¶”ì¶œ (3) ìµœí›„ í´ë°±(í•„í„° ì™„í™”)
# - Actions ë¡œê·¸ì— ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥(Top N)
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Optional, Dict, List, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs, urlunparse, unquote

import requests
import feedparser
from bs4 import BeautifulSoup


# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

SEARCH_DAYS = 14
MAX_ITEMS_PER_QUERY = 15   # ì¡°ê¸ˆ ëŠ˜ë¦¼(0ê±´ ë°©ì§€ì— ë„ì›€)

REQUEST_TIMEOUT = 15
USER_AGENT = "Mozilla/5.0 (NewsDigestBot/1.1; SlackWebhook)"
SLEEP_BETWEEN_REQUESTS = (0.15, 0.45)

SUMMARY_CHARS = 320
SLACK_TEXT_LIMIT = 3500

# Actions ë¡œê·¸ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ ê°œìˆ˜
PREVIEW_TOP_N = 20

# ë¦¬ì¡¸ë¸Œ ìºì‹œ
_RESOLVE_CACHE: Dict[str, str] = {}


# ==========================
# ìœ í‹¸
# ==========================
def _sleep() -> None:
    time.sleep(random.uniform(*SLEEP_BETWEEN_REQUESTS))

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _normalize_url(raw_url: str) -> str:
    """
    - google.com/url?q=... í˜•íƒœë©´ qì—ì„œ ì‹¤ URL ë³µì›
    - fragment ì œê±°
    - utm_* ì œê±°
    """
    if not raw_url:
        return raw_url

    # Google redirect URL ì²˜ë¦¬
    try:
        p = urlparse(raw_url)
        if p.netloc in ("www.google.com", "google.com") and p.path == "/url":
            q = parse_qs(p.query).get("q")
            if q and q[0]:
                raw_url = q[0]
    except Exception:
        pass

    # UTM ì œê±° + fragment ì œê±°
    try:
        p = urlparse(raw_url)
        qs = parse_qs(p.query)

        for k in list(qs.keys()):
            if k.lower().startswith("utm_"):
                qs.pop(k, None)

        parts = []
        for k, vs in qs.items():
            for v in vs:
                parts.append(f"{k}={v}")
        new_query = "&".join(parts)

        return urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, ""))
    except Exception:
        return raw_url

def _press_from_url(url: str) -> str:
    try:
        netloc = urlparse(url).netloc.lower().replace("www.", "")
        return netloc.split(".")[0].upper() if netloc else "NEWS"
    except Exception:
        return "NEWS"

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _google_news_rss_url(keyword: str, site: str, days: int) -> str:
    # ë”°ì˜´í‘œëŠ” ê²°ê³¼ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì–´ ì œê±° + when:Nd ìœ ì§€
    q = f"{keyword} site:{site} when:{days}d"
    return "https://news.google.com/rss/search?q=" + quote(q) + "&hl=ko&gl=KR&ceid=KR:ko"


def _extract_original_from_google_news_html(html: str) -> Optional[str]:
    """
    news.google.com/articles/... í˜ì´ì§€ì˜ HTMLì—ì„œ ì›ë³¸ ê¸°ì‚¬ URL ì¶”ì¶œ ì‹œë„.
    (ì¼€ì´ìŠ¤ì— ë”°ë¼ êµ¬ì¡°ê°€ ë°”ë€Œë¯€ë¡œ, ì—¬ëŸ¬ íŒíŠ¸ë¥¼ í­ë„“ê²Œ íƒìƒ‰)
    """
    soup = BeautifulSoup(html, "lxml")

    # 1) canonical / og:url
    for sel in [
        ("link", {"rel": "canonical"}, "href"),
        ("meta", {"property": "og:url"}, "content"),
    ]:
        tag = soup.find(sel[0], attrs=sel[1])
        if tag and tag.get(sel[2]):
            u = _clean_text(tag.get(sel[2]))
            if u and "news.google" not in u:
                return u

    # 2) a íƒœê·¸ ì¤‘ ì™¸ë¶€ https ë§í¬ ìš°ì„  íƒìƒ‰
    # êµ¬ê¸€ ë‰´ìŠ¤ í˜ì´ì§€ëŠ” ì™¸ë¶€ ë§í¬ê°€ /articles/... ë‚´ë¶€ ë¼ìš°íŒ…ì´ê±°ë‚˜ google.com/url?q= í˜•íƒœì¼ ìˆ˜ ìˆìŒ
    candidates: List[str] = []
    for a in soup.find_all("a", href=True):
        href = _clean_text(a["href"])
        if not href:
            continue

        # ìƒëŒ€ê²½ë¡œë©´ ìŠ¤í‚µ(ì›ë³¸ ì¶”ì¶œ ëª©ì )
        if href.startswith("/"):
            continue

        # google redirect í˜•ì‹ì´ë©´ q íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì‹±í•´ì„œ ì›ë³¸ìœ¼ë¡œ
        href = _normalize_url(href)

        if href.startswith("http") and ("news.google" not in href):
            candidates.append(href)

    # ê°€ì¥ ê·¸ëŸ´ë“¯í•œ(ê¸¸ì´ê°€ ê¸¸ê³  ì™¸ë¶€) ë§í¬ë¥¼ ë°˜í™˜
    if candidates:
        candidates = sorted(set(candidates), key=len, reverse=True)
        return candidates[0]

    return None


def resolve_final_url(raw_url: str, session: requests.Session, stats: Dict[str, int]) -> str:
    """
    1) allow_redirectsë¡œ ìµœì¢… URL ì¶”ì 
    2) ì—¬ì „íˆ news.googleì´ë©´ HTMLì„ GETí•´ì„œ ì›ë³¸ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
    """
    raw_url = _clean_text(raw_url)
    if not raw_url:
        return raw_url

    if raw_url in _RESOLVE_CACHE:
        return _RESOLVE_CACHE[raw_url]

    url = _normalize_url(raw_url)
    final_url = url

    # 1) redirect ë”°ë¼ê°€ê¸°
    try:
        resp = session.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True, stream=True)
        final_url = _normalize_url(resp.url)
        resp.close()
        stats["resolved_redirect"] += 1
    except Exception:
        final_url = url

    # 2) ìµœì¢…ì´ ì—¬ì „íˆ Google Newsë©´ HTML íŒŒì‹±ìœ¼ë¡œ ì›ë³¸ ë§í¬ ì¶”ì¶œ
    try:
        netloc = urlparse(final_url).netloc.lower()
        if "news.google" in netloc:
            stats["still_google_news_after_redirect"] += 1
            resp2 = session.get(final_url, timeout=REQUEST_TIMEOUT)
            resp2.raise_for_status()
            orig = _extract_original_from_google_news_html(resp2.text)
            if orig:
                final_url = _normalize_url(orig)
                stats["extracted_from_html"] += 1
    except Exception:
        pass

    _RESOLVE_CACHE[raw_url] = final_url
    return final_url


# ==========================
# ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
# ==========================
def extract_summary(url: str, session: requests.Session) -> str:
    """
    ìš°ì„ : trafilatura (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
    fallback: og/meta description + ì²« ë¬¸ë‹¨ ì¡°í•©
    """
    # 1) trafilatura (optional)
    try:
        import trafilatura  # type: ignore
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=False,
                favor_recall=False,
            )
            text = _clean_text(text)
            if text:
                return _truncate(text, SUMMARY_CHARS)
    except Exception:
        pass

    # 2) BeautifulSoup fallback
    try:
        resp = session.get(url, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "lxml")

        meta = soup.find("meta", attrs={"property": "og:description"}) or soup.find("meta", attrs={"name": "description"})
        desc = _clean_text(meta.get("content")) if meta and meta.get("content") else ""

        paras = []
        for p in soup.find_all("p"):
            t = _clean_text(p.get_text(" ", strip=True))
            if len(t) >= 35:
                paras.append(t)
            if len(" ".join(paras)) >= 900:
                break

        combined = _clean_text(" ".join([desc] + paras))
        return _truncate(combined, SUMMARY_CHARS) if combined else ""
    except Exception:
        return ""


# ==========================
# RSS ìˆ˜ì§‘
# ==========================
def fetch_articles() -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    articles: Dict[str, Dict] = {}
    stats = {
        "rss_entries_seen": 0,
        "resolved_redirect": 0,
        "still_google_news_after_redirect": 0,
        "extracted_from_html": 0,
        "date_filtered_out": 0,
        "domain_filtered_out": 0,
        "added": 0,
        "fallback_added_without_domain_match": 0,
    }

    for kw in PRIMARY_KEYWORDS:
        for site in TARGET_SITES:
            rss = _google_news_rss_url(kw, site, SEARCH_DAYS)
            try:
                resp = session.get(rss, timeout=REQUEST_TIMEOUT)
                resp.raise_for_status()
                feed = feedparser.parse(resp.text)

                got = 0
                for entry in feed.entries:
                    if got >= MAX_ITEMS_PER_QUERY:
                        break

                    stats["rss_entries_seen"] += 1

                    title = _clean_text(getattr(entry, "title", ""))
                    raw_link = _clean_text(getattr(entry, "link", ""))
                    published_dt = _parse_published(entry)

                    if not title or not raw_link:
                        continue

                    if not _within_days(published_dt, SEARCH_DAYS):
                        stats["date_filtered_out"] += 1
                        continue

                    final_link = resolve_final_url(raw_link, session, stats)

                    # ë„ë©”ì¸ í•„í„°: ì›ë³¸ ë§í¬ ê¸°ì¤€
                    if site not in final_link:
                        stats["domain_filtered_out"] += 1
                        continue

                    sid = _stable_id(title, final_link)
                    if sid in articles:
                        continue

                    articles[sid] = {
                        "keyword": kw,
                        "press": _press_from_url(final_link),
                        "title": title,
                        "link": final_link,
                        "published_dt": published_dt,
                        "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                        "summary": "",
                    }
                    stats["added"] += 1
                    got += 1

                _sleep()

            except Exception as e:
                print(f"[WARN] RSS ì‹¤íŒ¨ (kw={kw}, site={site}): {e}")
                continue

    # 0ê±´ì´ë©´: ìµœí›„ í´ë°± (ë„ë©”ì¸ í•„í„°ë¥¼ ì™„í™”í•´ì„œë¼ë„ ê²°ê³¼ë¥¼ í™•ë³´)
    # - ìš´ì˜ ìš”êµ¬ê°€ â€œë¬´ì¡°ê±´ ê¸°ì‚¬ ë³´ë‚´ê¸°â€ë¼ë©´, 0ê±´ì€ ì‹¤íŒ¨ì´ë¯€ë¡œ ìµœì†Œí•œ RSS ê²°ê³¼ë¼ë„ ë³´ë‚´ê²Œ í•¨
    if not articles:
        session2 = requests.Session()
        session2.headers.update({
            "User-Agent": USER_AGENT,
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
        })

        for kw in PRIMARY_KEYWORDS[:8]:  # í´ë°±ì€ ê³¼ë„í•œ íŠ¸ë˜í”½ ë°©ì§€ ìœ„í•´ ì¼ë¶€ í‚¤ì›Œë“œë§Œ
            rss = "https://news.google.com/rss/search?q=" + quote(f"{kw} when:{SEARCH_DAYS}d") + "&hl=ko&gl=KR&ceid=KR:ko"
            try:
                resp = session2.get(rss, timeout=REQUEST_TIMEOUT)
                resp.raise_for_status()
                feed = feedparser.parse(resp.text)
                for entry in feed.entries[:25]:
                    title = _clean_text(getattr(entry, "title", ""))
                    raw_link = _clean_text(getattr(entry, "link", ""))
                    published_dt = _parse_published(entry)
                    if not title or not raw_link:
                        continue
                    if not _within_days(published_dt, SEARCH_DAYS):
                        continue

                    final_link = resolve_final_url(raw_link, session2, stats)
                    sid = _stable_id(title, final_link)
                    if sid in articles:
                        continue

                    articles[sid] = {
                        "keyword": kw,
                        "press": _press_from_url(final_link),
                        "title": title,
                        "link": final_link,
                        "published_dt": published_dt,
                        "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                        "summary": "",
                    }
                    stats["fallback_added_without_domain_match"] += 1

                _sleep()
            except Exception:
                continue

    # ìš”ì•½ ì±„ìš°ê¸°
    for a in articles.values():
        a["summary"] = extract_summary(a["link"], session)
        _sleep()

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats


# ==========================
# Slack ë©”ì‹œì§€ ìƒì„±/ì „ì†¡
# ==========================
def is_nexon(article: Dict) -> bool:
    blob = f"{article.get('title','')} {article.get('summary','')} {article.get('link','')}".lower()
    return ("ë„¥ìŠ¨" in blob) or ("nexon" in blob)

def build_messages(articles: List[Dict], stats: Dict[str, int]) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {SEARCH_DAYS}ì¼)\n"
    header += f"- stats: entries={stats.get('rss_entries_seen',0)}, added={stats.get('added',0)}, fallback={stats.get('fallback_added_without_domain_match',0)}\n"
    header += f"- resolve: redirect={stats.get('resolved_redirect',0)}, still_google={stats.get('still_google_news_after_redirect',0)}, html_extract={stats.get('extracted_from_html',0)}\n"
    header += f"- filtered: domain={stats.get('domain_filtered_out',0)}, date={stats.get('date_filtered_out',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        summ = f"\n    - {_truncate(a.get('summary',''), 500)}" if a.get("summary") else ""
        return f"â–¶ *[{a['press']}]* <{a['link']}|{a['title']}>{pub}{summ}\n"

    major = articles
    nexon = [a for a in articles if is_nexon(a)]

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not major:
        body += f"- ìµœê·¼ {SEARCH_DAYS}ì¼ê°„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in major:
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤\n"
    if not nexon:
        body += "- 'ë„¥ìŠ¨' ê´€ë ¨ ê¸°ì‚¬(ì œëª©/ìš”ì•½/URL ê¸°ì¤€)ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon:
            body += fmt(a)

    full = header + body

    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()


def main() -> None:
    articles, stats = fetch_articles()

    # Actions ë¡œê·¸ ë¯¸ë¦¬ë³´ê¸° (Slack ì˜¤ê¸° ì „ì—ë„ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥)
    print(f"[INFO] fetched articles: {len(articles)}")
    print(f"[INFO] stats: {stats}")
    print("[INFO] preview:")
    for i, a in enumerate(articles[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','')}] {a.get('title','')} :: {a.get('link','')}")

    # Slack ì „ì†¡
    messages = build_messages(articles, stats)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.4)

if __name__ == "__main__":
    main()
