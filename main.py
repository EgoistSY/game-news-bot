# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4] FAST Google News RSS -> Slack Digest (2026-02-23)
# ëª©í‘œ: 1~2ë¶„ ë‚´ ì™„ë£Œ + 0ê±´ í™•ë¥  ìµœì†Œí™” (Python 3.9 í˜¸í™˜)
#
# ì „ëµ:
# - googlesearch-python ì œê±°
# - news.google ë§í¬ ë¦¬ì¡¸ë¸Œ/HTML íŒŒì‹±/ë³¸ë¬¸ í¬ë¡¤ë§ ì œê±° (ì†ë„â†‘, ì•ˆì •â†‘)
# - RSS ì¿¼ë¦¬ ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ site: OR ë¬¶ìŒ ì‚¬ìš©
# - 0ê±´ì´ë©´ ìë™ìœ¼ë¡œ í•„í„° ì™„í™” í´ë°± ì‹¤í–‰
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote

import requests
import feedparser

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

SEARCH_DAYS = 14

# ì†ë„/ì•ˆì • ë°¸ëŸ°ìŠ¤
# - í‚¤ì›Œë“œ ì „ë¶€ë¥¼ ë‹¤ ë•Œë¦¬ë©´ RSS í˜¸ì¶œì´ ëŠ˜ì–´ë‚¨
# - ìš´ì˜ìš©ì—ì„œëŠ” ìƒìœ„ Nê°œë§Œ ë¨¼ì € ìˆ˜ì§‘í•˜ê³ , 0ê±´ì´ë©´ í™•ì¥ í´ë°±
KEYWORD_BATCH_PRIMARY = 10   # 1ì°¨: ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
KEYWORD_BATCH_FALLBACK = 18  # 2ì°¨(0ê±´ì¼ ë•Œ): ì „ì²´ í‚¤ì›Œë“œ

# RSS 1íšŒ í˜¸ì¶œì—ì„œ ìµœëŒ€ ëª‡ ê°œ entryê¹Œì§€ ì‚¬ìš©í• ì§€
MAX_ENTRIES_PER_FEED = 30

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.0; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.15)

# Slack ë©”ì‹œì§€ ì œí•œ ëŒ€ì‘
SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180

# Actions ë¡œê·¸ ë¯¸ë¦¬ë³´ê¸°
PREVIEW_TOP_N = 20


# ==========================
# ìœ í‹¸
# ==========================
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess_from_source(entry) -> str:
    # feedparserê°€ source/title ë“±ì„ ì£¼ëŠ” ê²½ìš°ê°€ ìˆìŒ. ì—†ìœ¼ë©´ NEWS.
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    # Google News RSS Search
    # hl=ko gl=KR ceid=KR:ko ê³ ì •
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    # (site:a OR site:b OR site:c)
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _build_query(keyword: str, sites: List[str], days: int) -> str:
    # keyword + (site OR ...) + when:Nd
    # ë”°ì˜´í‘œëŠ” ê²°ê³¼ë¥¼ ê¸‰ê°ì‹œí‚¬ ìˆ˜ ìˆì–´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    return f"{keyword} {_site_or_query(sites)} when:{days}d"


# ==========================
# RSS ìˆ˜ì§‘ (ë¹ ë¥¸ ë²„ì „)
# ==========================
def fetch_fast(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    # í‚¤ì›Œë“œë³„ë¡œ RSS í•œ ë²ˆì”©ë§Œ í˜¸ì¶œ (ì‚¬ì´íŠ¸ëŠ” ORë¡œ ë¬¶ìŒ)
    for kw in keywords:
        q = _build_query(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            entries = feed.entries[:MAX_ENTRIES_PER_FEED]

            for e in entries:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                # RSSì—ì„œ ì œê³µí•˜ëŠ” summaryê°€ ìˆì„ ìˆ˜ ìˆìŒ(ì§§ê²Œë§Œ ì‚¬ìš©)
                snippet = _clean_text(getattr(e, "summary", "") or getattr(e, "description", ""))

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "keyword": kw,
                    "press": _press_guess_from_source(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": _truncate(snippet, SNIPPET_MAX) if snippet else "",
                }
                stats["added"] += 1

            _sleep()

        except Exception as ex:
            print(f"[WARN] RSS call failed (kw={kw}): {ex}")
            continue

    # ìµœì‹ ìˆœ ì •ë ¬
    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats


# ==========================
# Slack ë©”ì‹œì§€ ìƒì„±/ì „ì†¡
# ==========================
def _is_nexon(a: Dict) -> bool:
    blob = f"{a.get('title','')} {a.get('snippet','')} {a.get('link','')}".lower()
    return ("ë„¥ìŠ¨" in blob) or ("nexon" in blob)

def build_messages(articles: List[Dict], stats: Dict[str, int], days: int) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {days}ì¼)\n"
    header += f"- ìˆ˜ì§‘: feeds={stats.get('feeds_called',0)}, entries={stats.get('entries_seen',0)}, added={stats.get('added',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    major = articles
    nexon = [a for a in articles if _is_nexon(a)]

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not major:
        body += f"- ìµœê·¼ {days}ì¼ ê¸°ì¤€ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in major[:80]:  # ë„ˆë¬´ ë§ì´ ë³´ë‚´ë©´ ìŠ¤íŒ¸ì´ë¯€ë¡œ ìƒí•œ
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤\n"
    if not nexon:
        body += "- 'ë„¥ìŠ¨' ê´€ë ¨ ê¸°ì‚¬(ì œëª©/ìš”ì•½/URL ê¸°ì¤€)ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon[:50]:
            body += fmt(a)

    full = header + body

    # Slack ê¸¸ì´ ì œí•œ ëŒ€ì‘: ë¼ì¸ ë‹¨ìœ„ ë¶„í• 
    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()


# ==========================
# Main (ìë™ í´ë°±)
# ==========================
def main() -> None:
    # 1ì°¨ ì‹œë„: ìƒìœ„ í‚¤ì›Œë“œë§Œ ë¹ ë¥´ê²Œ
    primary_keywords = PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY]
    articles, stats = fetch_fast(primary_keywords, TARGET_SITES, SEARCH_DAYS)

    # 0ê±´ì´ë©´ 2ì°¨(í‚¤ì›Œë“œ í™•ì¥)
    if not articles:
        print("[INFO] primary fetch returned 0. fallback to full keyword set.")
        articles, stats = fetch_fast(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, SEARCH_DAYS)

    # ê·¸ë˜ë„ 0ê±´ì´ë©´ ìµœí›„ í´ë°±: ì‚¬ì´íŠ¸ í•„í„° ì œê±°(ì—…ê³„ ë‰´ìŠ¤ë¼ë„ ë³´ë‚´ê¸°)
    if not articles:
        print("[INFO] still 0. final fallback: remove site filters.")
        # í‚¤ì›Œë“œ 10ê°œë§Œ, whenë§Œ ìœ ì§€
        session_sites: List[str] = []
        articles, stats = fetch_fast(PRIMARY_KEYWORDS[:10], session_sites, SEARCH_DAYS)

    print(f"[INFO] fetched articles: {len(articles)}")
    print(f"[INFO] stats: {stats}")
    print("[INFO] preview:")
    for i, a in enumerate(articles[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    # Slack ì „ì†¡
    messages = build_messages(articles, stats, SEARCH_DAYS)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.2)

if __name__ == "__main__":
    main()
