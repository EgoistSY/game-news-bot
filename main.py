# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.4.0] FAST Google News RSS -> Slack Digest
# - Python 3.9 í˜¸í™˜
# - v4.4.0 ê°œì„ ì‚¬í•­:
#   1) get_canonical_link(): Google ì¤‘ê°„ ë§í¬ì—ì„œ ì›ë¬¸ URL ì‹¤ì œ ë””ì½”ë”©
#      -> ì¶”ê°€ HTTP ìš”ì²­ ì—†ì´ URL íŒŒë¼ë¯¸í„°(url=) íŒŒì‹±ìœ¼ë¡œ ì›ë¬¸ ì¶”ì¶œ
#   2) is_valid_article_url() ì¸ë²¤ í•„í„° ëŒ€í­ ê°•í™”
#      -> /webzine/news?news=ìˆ«ìž íŒ¨í„´ë§Œ í—ˆìš©
#   3) ë¹„ê¸°ì‚¬ ì œëª© íŒ¨í„´ í•„í„° ì¶”ê°€
#      -> ê°€ì´ë“œ, ëª¨ì§‘, ìŠ¤í¬ì£¼ì˜, LCK ê²½ê¸°ê²°ê³¼(ìŠ¹/íŒ¨) ë“± ì œëª© ê¸°ë°˜ ì°¨ë‹¨
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, date
from urllib.parse import quote, urlparse, parse_qs, unquote

import requests
import feedparser

# Python 3.9 zoneinfo
from zoneinfo import ZoneInfo

SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ìž‘", "ì„±ê³¼", "í˜¸ìž¬", "ì•…ìž¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ìž", "M&A"
]

SEARCH_DAYS = 1

KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18

MAX_ENTRIES_PER_FEED = 30
MAX_ENTRIES_PER_NEXON_FEED = 20

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.32; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.12)

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 12

# --------------------------
# ì»¨í…ìŠ¤íŠ¸/í•„í„°
# --------------------------
GAME_CONTEXT_OR = [
    "ê²Œìž„", "ê²Œì´ë°", "ê²Œìž„ì—…ê³„", "ê²Œìž„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œìž„", "PCê²Œìž„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports"
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

STRICT_SITES = {"zdnet.co.kr", "ddaily.co.kr"}

GAME_HINTS = [
    "ê²Œìž„", "ê²Œì´ë°", "ì‹ ìž‘", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ", "ìŠ¤íŒ€", "ì½˜ì†”", "ëª¨ë°”ì¼", "PC",
    "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox", "RPG", "MMORPG", "FPS", "MOBA",
    "eìŠ¤í¬ì¸ ", "esports",
    "ë„¥ìŠ¨", "ì—”ì”¨", "í¬ëž˜í”„í†¤", "ë„·ë§ˆë¸”", "ì¹´ì¹´ì˜¤ê²Œìž„", "ìŠ¤ë§ˆì¼ê²Œì´íŠ¸", "íŽ„ì–´ë¹„ìŠ¤",
]

NEXON_TERMS = [
    "ë„¥ìŠ¨", "nexon",
    "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œìž„ì¦ˆ", "ë„¥ìŠ¨ ë„¤íŠ¸ì›ìŠ¤", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤",
    "ë„¤ì˜¤í”Œ", "ë„¥ìŠ¨GT", "ë„¥ìŠ¨ì§€í‹°",
]

NEXON_IMPORTANCE = [
    ("M&A", 5), ("ì¸ìˆ˜", 5), ("í•©ë³‘", 5),
    ("íˆ¬ìž", 4), ("ì§€ë¶„", 4),
    ("ì†Œì†¡", 5), ("ê·œì œ", 4),
    ("ë§¤ì¶œ", 4), ("ì‹¤ì ", 4), ("ì˜ì—…ì´ìµ", 4), ("ìˆœì´ìµ", 4),
    ("ì¶œì‹œ", 3), ("ì—…ë°ì´íŠ¸", 3),
    ("CBT", 2), ("OBT", 2),
    ("ë¦¬ìŠ¤í¬", 3), ("ì•…ìž¬", 3), ("í˜¸ìž¬", 3),
]

# --------------------------
# âœ… ë¹„ê¸°ì‚¬ ì œëª© íŒ¨í„´ í•„í„°
#    ì•„ëž˜ ì •ê·œì‹ ì¤‘ í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ ê±¸ëŸ¬ëƒ„
# --------------------------
NON_ARTICLE_TITLE_PATTERNS = [
    # ì¸ë²¤ ê²Œì‹œíŒ ìœ í˜•
    re.compile(r"^\[ëª¨ì§‘\]"),          # ê¸¸ë“œ/íŒŒí‹° ëª¨ì§‘
    re.compile(r"^\(ìŠ¤í¬ì£¼ì˜\)"),       # ìŠ¤í¬ì¼ëŸ¬ í¬í•¨ ì»¤ë®¤ë‹ˆí‹° ê¸€
    re.compile(r"^ì›¹ì§„\s*$"),           # ë‹¨ìˆœ "ì›¹ì§„" ì œëª©
    # eìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ ë‹¨ì‹  (ì—…ê³„ ë‰´ìŠ¤ ëª©ì ì— ë¶ˆí•„ìš”)
    re.compile(r"\[LCK"),              # LCK ê²½ê¸° ê´€ë ¨
    re.compile(r"\[ë¡¤ì±”ìŠ¤\]"),
    re.compile(r"\[ì˜¤ë²„ì›Œì¹˜\s*ë¦¬ê·¸\]"),
    # ì»¤ë®¤ë‹ˆí‹°ì„± ê°€ì´ë“œ/ê³µëžµ
    re.compile(r"ê°€ì´ë“œ\s*\d+\.?\d*v"),  # "í‚¤ì„¸íŒ… ì„¤ì • ê°€ì´ë“œ 1.0v" ë“±
    re.compile(r"^\[ê³µëžµ\]"),
]

def has_non_article_title(title: str) -> bool:
    """ì œëª©ì´ ë¹„ê¸°ì‚¬ íŒ¨í„´ì— í•´ë‹¹í•˜ë©´ True"""
    for pat in NON_ARTICLE_TITLE_PATTERNS:
        if pat.search(title):
            return True
    return False


# --------------------------
# âœ… í•µì‹¬ ìˆ˜ì •: ì›ë¬¸ URL ì¶”ì¶œ (ì¶”ê°€ HTTP ìš”ì²­ ì—†ìŒ)
# --------------------------
def get_canonical_link(entry) -> str:
    """
    Google News RSS entry.linkëŠ” ë³´í†µ ì•„ëž˜ ë‘ í˜•íƒœ ì¤‘ í•˜ë‚˜:
      A) https://news.google.com/rss/articles/...  (ë¶ˆíˆ¬ëª… ID)
      B) https://news.google.com/articles/...?hl=...
      C) ì¼ë¶€ feedparser ë²„ì „ì—ì„œ entry.source ì— ì›ë¬¸ URL ì œê³µ

    ìš°ì„ ìˆœìœ„:
      1) entry.links ì¤‘ typeì´ text/html ì´ê³  Google ë„ë©”ì¸ì´ ì•„ë‹Œ ê²ƒ
      2) entry.source ì˜ href/url
      3) entry.link ì˜ ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì—ì„œ url= / q= íŒŒë¼ë¯¸í„° íŒŒì‹±
      4) entry.link ì›ë³¸ (fallback)
    """
    # 1) entry.links ìˆœíšŒ â€” Google ë„ë©”ì¸ì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ë§í¬
    try:
        links = getattr(entry, "links", []) or []
        for lk in links:
            href = _clean_text(lk.get("href", "") if isinstance(lk, dict) else getattr(lk, "href", ""))
            if href and "google.com" not in href:
                return href
    except Exception:
        pass

    # 2) entry.source ì˜ href / url
    try:
        src = getattr(entry, "source", None)
        if src:
            for k in ("href", "url"):
                val = (_clean_text(src.get(k, "")) if isinstance(src, dict)
                       else _clean_text(getattr(src, k, "") or ""))
                if val and "google.com" not in val:
                    return val
    except Exception:
        pass

    # 3) entry.link ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì—ì„œ ì›ë¬¸ URL íŒŒë¼ë¯¸í„° ì‹œë„
    raw_link = _clean_text(getattr(entry, "link", "") or "")
    if raw_link:
        try:
            p = urlparse(raw_link)
            qs = parse_qs(p.query or "")
            for param in ("url", "q", "u"):
                vals = qs.get(param, [])
                if vals:
                    decoded = unquote(vals[0])
                    if decoded.startswith("http") and "google.com" not in decoded:
                        return decoded
        except Exception:
            pass

    # 4) fallback: entry.link ê·¸ëŒ€ë¡œ (Google ì¤‘ê°„ ë§í¬ì¼ ìˆ˜ ìžˆìŒ)
    return raw_link


# --------------------------
# âœ… ê°•í™”ëœ URL í•„í„° (ì›ë¬¸ URL ê¸°ì¤€)
# --------------------------
def is_valid_article_url(url: str) -> bool:
    if not url:
        return False

    try:
        p = urlparse(url)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        qs = parse_qs(p.query or "")
    except Exception:
        return True

    # Google ì¤‘ê°„ ë§í¬ëŠ” ì›ë¬¸ URLì´ ì•„ë‹ˆë¯€ë¡œ ì›ì¹™ì ìœ¼ë¡œ ê±¸ëŸ¬ëƒ„
    # (fallbackìœ¼ë¡œ ë‚¨ì€ ê²½ìš° í†µê³¼ì‹œì¼œ ë‚˜ì¤‘ì— ì œëª© í•„í„°ì—ì„œ ì²˜ë¦¬)
    if "news.google.com" in host:
        # íŒë‹¨ ë¶ˆê°€ â€” ì¼ë‹¨ í†µê³¼ì‹œí‚¤ë˜ ë¡œê·¸ ë‚¨ê¹€ (ì›ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
        return True

    # ê³µí†µ ë¹„ê¸°ì‚¬ ê²½ë¡œ
    common_bad_tokens = [
        "/board/",
        "/search",
        "/tag/",
        "/ranking", "/rank",
        "/gallery",
        "/forum/",
        "/community/",
    ]
    if any(tok in path for tok in common_bad_tokens):
        return False

    # âœ… Inven íŠ¹í™” â€” ë‰´ìŠ¤ ê¸°ì‚¬ URLë§Œ í—ˆìš©
    if host.endswith("inven.co.kr"):
        # í—ˆìš© íŒ¨í„´: /webzine/news?news=ìˆ«ìž (ì‹¤ì œ ê¸°ì‚¬)
        #   ì˜ˆ) https://www.inven.co.kr/webzine/news/?news=298765
        if path.rstrip("/") == "/webzine/news" or path.startswith("/webzine/news"):
            news_ids = qs.get("news", [])
            if news_ids and re.match(r"^\d+$", news_ids[0]):
                return True  # ì •ìƒ ê¸°ì‚¬
            else:
                return False  # í‚¤ì›Œë“œ ëª©ë¡, ì›¹ì§„ ë©”ì¸ ë“±
        # ê·¸ ì™¸ inven ê²½ë¡œëŠ” ëª¨ë‘ ì°¨ë‹¨ (ê²Œì‹œíŒ, ê°¤ëŸ¬ë¦¬ ë“±)
        return False

    return True


# --------------------------
# ìœ í‹¸
# --------------------------
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess(entry) -> str:
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _has_any_hint(text: str, hints: List[str]) -> bool:
    blob = (text or "").lower()
    return any(h.lower() in blob for h in hints)

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

def nexon_score(article: Dict) -> int:
    blob = f"{article.get('title','')} {article.get('snippet','')}".lower()
    score = 0
    for kw, w in NEXON_IMPORTANCE:
        if kw.lower() in blob:
            score += w
    if contains_nexon(article.get("title", ""), article.get("snippet", "")):
        score += 2
    return score

# --------------------------
# ì „ë‚ (KST) ë‚ ì§œ ë²”ìœ„
# --------------------------
def yesterday_range_kst() -> Tuple[str, str, str]:
    tz = ZoneInfo("Asia/Seoul")
    now = datetime.now(tz)
    today = now.date()
    yday = today - timedelta(days=1)
    return (yday.isoformat(), yday.isoformat(), today.isoformat())

# --------------------------
# ì¿¼ë¦¬
# --------------------------
def build_query_general(keyword: str, sites: List[str], after: str, before: str) -> str:
    if sites:
        return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} after:{after} before:{before}"
    return f"{GAME_CONTEXT_QUERY} {keyword} after:{after} before:{before}"

def build_query_nexon(keyword: str, sites: List[str], after: str, before: str) -> str:
    nexon_expr = '("ë„¥ìŠ¨" OR Nexon OR "ë„¥ìŠ¨ê²Œìž„ì¦ˆ" OR ë„¤ì˜¤í”Œ)'
    if sites:
        return f'{nexon_expr} {keyword} {_site_or_query(sites)} after:{after} before:{before}'
    return f'{nexon_expr} {keyword} after:{after} before:{before}'

# --------------------------
# RSS ìˆ˜ì§‘ â€” ê³µí†µ ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ë¡œì§
# --------------------------
def _process_entry(e, stats: Dict, track: str, kw: str) -> Optional[Dict]:
    """
    ë‹¨ì¼ RSS ì—”íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ dict ë°˜í™˜.
    í•„í„°ì— ê±¸ë¦¬ë©´ None ë°˜í™˜ + stats ì—…ë°ì´íŠ¸.
    """
    title = _clean_text(getattr(e, "title", ""))
    if not title:
        return None

    # âœ… ë¹„ê¸°ì‚¬ ì œëª© íŒ¨í„´ í•„í„°
    if has_non_article_title(title):
        stats.setdefault("title_pattern_filtered_out", 0)
        stats["title_pattern_filtered_out"] += 1
        return None

    # âœ… ì›ë¬¸ URL ì¶”ì¶œ
    link = get_canonical_link(e)
    if not link:
        return None

    # âœ… URL ê¸°ë°˜ ë¹„ê¸°ì‚¬ í•„í„°
    if not is_valid_article_url(link):
        stats["non_article_url_filtered_out"] += 1
        return None

    published_dt = _parse_published(e)
    if not _within_days(published_dt, SEARCH_DAYS):
        stats["date_filtered_out"] += 1
        return None

    snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
    snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

    # strict ì‚¬ì´íŠ¸(zdnet, ddaily) â€” ê²Œìž„ ížŒíŠ¸ ì—†ìœ¼ë©´ ì œì™¸
    if any(s in link for s in STRICT_SITES) or any(s in title for s in ("ì§€ë””ë„·", "ë””ì§€í„¸ë°ì¼ë¦¬")):
        if not _has_any_hint(f"{title} {snippet}", GAME_HINTS):
            stats["strict_filtered_out"] = stats.get("strict_filtered_out", 0) + 1
            return None

    return {
        "track": track,
        "keyword": kw,
        "press": _press_guess(e),
        "title": _truncate(title, TITLE_MAX),
        "link": link,
        "published_dt": published_dt,
        "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
        "snippet": snippet,
    }


def fetch_general(keywords: List[str], sites: List[str], after: str, before: str) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats: Dict[str, int] = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "strict_filtered_out": 0,
        "non_article_url_filtered_out": 0,
        "title_pattern_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_general(kw, sites, after, before)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_FEED]:
                stats["entries_seen"] += 1
                article = _process_entry(e, stats, "general", kw)
                if article is None:
                    continue

                sid = _stable_id(article["title"], article["link"])
                if sid in articles:
                    continue

                articles[sid] = article
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (general kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats


def fetch_nexon(keywords: List[str], sites: List[str], after: str, before: str) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats: Dict[str, int] = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "nexon_filtered_out": 0,
        "non_article_url_filtered_out": 0,
        "title_pattern_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_nexon(kw, sites, after, before)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_NEXON_FEED]:
                stats["entries_seen"] += 1
                article = _process_entry(e, stats, "nexon", kw)
                if article is None:
                    continue

                # ë„¥ìŠ¨ ìµœì¢… ê²€ì¦ (ì œëª©/ìš”ì•½ì— ì‹¤ì œ ë„¥ìŠ¨ í¬í•¨)
                if not contains_nexon(article["title"], article["snippet"]):
                    stats["nexon_filtered_out"] += 1
                    continue

                sid = _stable_id(article["title"], article["link"])
                if sid in articles:
                    continue

                articles[sid] = article
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (nexon kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats


# --------------------------
# Slack ë©”ì‹œì§€
# --------------------------
def build_messages(general: List[Dict], nexon: List[Dict],
                   stats_g: Dict[str, int], stats_n: Dict[str, int],
                   yday_label: str) -> List[str]:
    header = f"## ðŸ“° {yday_label} ì „ë‚  ì£¼ìš” ê²Œìž„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ë°œì†¡: KST 10:00)\n"
    header += (
        f"- general: feeds={stats_g.get('feeds_called',0)}, "
        f"entries={stats_g.get('entries_seen',0)}, added={stats_g.get('added',0)}, "
        f"strict_drop={stats_g.get('strict_filtered_out',0)}, "
        f"non_article_drop={stats_g.get('non_article_url_filtered_out',0)}, "
        f"title_pat_drop={stats_g.get('title_pattern_filtered_out',0)}\n"
    )
    header += (
        f"- nexon: feeds={stats_n.get('feeds_called',0)}, "
        f"entries={stats_n.get('entries_seen',0)}, added={stats_n.get('added',0)}, "
        f"nexon_drop={stats_n.get('nexon_filtered_out',0)}, "
        f"non_article_drop={stats_n.get('non_article_url_filtered_out',0)}, "
        f"title_pat_drop={stats_n.get('title_pattern_filtered_out',0)}\n\n"
    )

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    body = "### ðŸŒ ì£¼ìš” ê²Œìž„ì—…ê³„ ë‰´ìŠ¤\n"
    if not general:
        body += "- ì „ë‚  ê¸°ì¤€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in general[:70]:
            body += fmt(a)

    body += "\n---\n### ðŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ (Top 5)\n"
    if not nexon:
        body += "- ë„¥ìŠ¨ ê´€ë ¨ ë‰´ìŠ¤(í‚¤ì›Œë“œ êµì§‘í•© + ì œëª©/ìš”ì•½ ê²€ì¦)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
    else:
        scored = sorted(
            nexon,
            key=lambda x: (nexon_score(x), x["published_dt"] or datetime.min),
            reverse=True
        )
        for a in scored[:5]:
            body += fmt(a)

    full = header + body

    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)
    return messages


def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()


# --------------------------
# Main
# --------------------------
def main() -> None:
    yday_label, after, before = yesterday_range_kst()

    general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY], TARGET_SITES, after, before)
    if not general:
        general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, after, before)

    nexon, stats_n = fetch_nexon(PRIMARY_KEYWORDS, TARGET_SITES, after, before)

    print(f"[INFO] date_range_kst: after={after}, before={before} (yday={yday_label})")
    print(f"[INFO] general fetched: {len(general)}, stats: {stats_g}")
    print(f"[INFO] nexon fetched: {len(nexon)}, stats: {stats_n}")
    print("[INFO] preview general:")
    for i, a in enumerate(general[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    messages = build_messages(general, nexon, stats_g, stats_n, yday_label)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.15)


if __name__ == "__main__":
    main()
