# ------------------------------------------------------------------
# [ìš´ì˜ìš© v5] "ì§„ì§œ ê¸°ì‚¬"ë§Œ + "ì§„ì§œ ì›ë¬¸ ë§í¬"ë§Œ + KST 10ì‹œ ê¸°ì¤€ ì „ ì˜ì—…ì¼ ìœˆë„ìš°
# - Google News / googlesearch ì™„ì „ ì œê±° (news.google.com ë§í¬ ì›ì²œ ì°¨ë‹¨)
# - ì¸ë²¤: RSS(FeedBurner) ì‚¬ìš©
# - ê²Œì„ë©”ì¹´/ê²Œì„í”Œ/ê²Œì„í†¡: HTML ë¦¬ìŠ¤íŠ¸ì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘
# - ê¸°ì‚¬ ê²€ì¦: (1) URL íŒ¨í„´ (ë„ë©”ì¸ë³„) + (2) ì œëª© íŒíŠ¸
# - ê¸°ê°„: "ì „ ì˜ì—…ì¼ 00:00 ~ ì˜¤ëŠ˜ 09:59" (ì£¼ë§/ê³µíœ´ì¼ ë¡¤ë°±)
#
# requirements.txt:
#   requests
#   feedparser
# ------------------------------------------------------------------

import os
import re
import json
import time
import random
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, date
from urllib.parse import urlparse, parse_qs, urljoin

import requests
import feedparser
from zoneinfo import ZoneInfo

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

KST = ZoneInfo("Asia/Seoul")
USER_AGENT = "Mozilla/5.0 (GameNewsBot/5.0; +https://github.com/)"
TIMEOUT = 12

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

# Slack ì¶œë ¥ ì œí•œ
GENERAL_SEND_LIMIT = 40
NEXON_SEND_LIMIT = 5
SLACK_TEXT_LIMIT = 3500

# ì¸ë²¤ RSS (FeedBurner)
INVEN_RSS = "https://feeds.feedburner.com/inven"

# HTML ë¦¬ìŠ¤íŠ¸ ì†ŒìŠ¤
GAMEMECA_LIST = "https://www.gamemeca.com/news.php"
GAMEPLE_HOME = "https://www.gameple.co.kr/"
GAMETOC_LIST = "https://www.gametoc.co.kr/news/articleList.html?view_type=sm"

# ê¸°ì‚¬ ì•„ë‹Œ ê¸€ íŒíŠ¸(ì œëª© ê¸°ë°˜)
NON_ARTICLE_TITLE_HINTS = [
    "ê³µëµ", "íŒ", "ë…¸í•˜ìš°", "ì§ˆë¬¸", "Q&A", "ì¸ì¦", "í›„ê¸°", "ìŠ¤ìƒ·", "ìŠ¤í¬ë¦°ìƒ·",
    "ê¸¸ë“œ", "ê¸¸ë“œëª¨ì§‘", "ëª¨ì§‘", "íŒŒí‹°", "íŒŸ", "ê³ ì •íŒŸ", "í´ëœ", "í´ëœëª¨ì§‘",
    "ê±°ë˜", "ë‚˜ëˆ”", "íŒë§¤", "ì‚½ë‹ˆë‹¤", "ë²„ê·¸ì œë³´", "ê±´ì˜", "í† ë¡ ",
]

NEXON_TERMS = ["ë„¥ìŠ¨", "nexon", "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œì„ì¦ˆ", "ë„¤ì˜¤í”Œ", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤"]


# ==========================
# 2026 KR ê³µíœ´ì¼ (í•˜ë“œì½”ë”©, ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆí•„ìš”)
# ì¶œì²˜: VisitKorea 2026 Public Holidays í‘œ ê¸°ë°˜
# ==========================
KR_HOLIDAYS_2026 = {
    date(2026, 1, 1),
    date(2026, 2, 16), date(2026, 2, 17), date(2026, 2, 18),  # ì„¤ë‚ 
    date(2026, 3, 1), date(2026, 3, 2),  # ì‚¼ì¼ì ˆ(+ëŒ€ì²´)
    date(2026, 5, 5),  # ì–´ë¦°ì´ë‚ 
    date(2026, 5, 24), date(2026, 5, 25),  # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚ (+ëŒ€ì²´)
    date(2026, 6, 3),  # ì§€ë°©ì„ ê±°
    date(2026, 6, 6),  # í˜„ì¶©ì¼
    date(2026, 8, 15), date(2026, 8, 17),  # ê´‘ë³µì ˆ(+ëŒ€ì²´)
    date(2026, 9, 24), date(2026, 9, 25), date(2026, 9, 26),  # ì¶”ì„
    date(2026, 10, 3), date(2026, 10, 5),  # ê°œì²œì ˆ(+ëŒ€ì²´)
    date(2026, 10, 9),  # í•œê¸€ë‚ 
    date(2026, 12, 25),  # ì„±íƒ„ì ˆ
}

# ==========================
# ìœ í‹¸
# ==========================
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def looks_like_non_article(title: str) -> bool:
    t = (title or "").lower()
    return any(h.lower() in t for h in NON_ARTICLE_TITLE_HINTS)

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

def is_business_day(d: date) -> bool:
    # ì›”(0)~ê¸ˆ(4) and not holiday
    if d.weekday() >= 5:
        return False
    if d.year == 2026 and d in KR_HOLIDAYS_2026:
        return False
    return True

def compute_window(now_kst: datetime) -> Tuple[datetime, datetime]:
    """
    ì‹¤í–‰ ì‹œê°ì´ (ëŒ€ì²´ë¡œ) KST 10:00ì´ë¼ê³  ê°€ì •.
    ìœˆë„ìš°: ì „ ì˜ì—…ì¼ 00:00 ~ ì˜¤ëŠ˜ 09:59:59
    ë‹¨, ì˜¤ëŠ˜ì´ ì˜ì—…ì¼ì´ ì•„ë‹ˆë©´ ì˜¤ëŠ˜ë„ ë¡¤ë°±í•´ì„œ 'ë§ˆì§€ë§‰ ì˜ì—…ì¼'ì˜ ë‹¤ìŒë‚  09:59ê¹Œì§€ë¡œ ì¡ëŠ”ë‹¤.
    """
    # ì˜¤ëŠ˜ 09:59:59 (KST)
    end = now_kst.replace(hour=9, minute=59, second=59, microsecond=0)

    # ì˜¤ëŠ˜ì´ ì˜ì—…ì¼ì´ ì•„ë‹ˆë©´ end ìì²´ë¥¼ "ì˜ì—…ì¼ ë‹¤ìŒë‚  09:59"ë¡œ ë§ì¶”ê¸° ìœ„í•´
    # now_kstì˜ ë‚ ì§œë¥¼ ì˜ì—…ì¼ì´ ë  ë•Œê¹Œì§€ ë’¤ë¡œ ë¯¼ë‹¤.
    base_day = now_kst.date()
    while not is_business_day(base_day):
        base_day = base_day - timedelta(days=1)

    # endë¥¼ base_dayì˜ "ë‹¤ìŒë‚  09:59"ë¡œ ë³´ì • (ì¦‰, base_day ì»¤ë²„ ë)
    end = datetime(base_day.year, base_day.month, base_day.day, 9, 59, 59, tzinfo=KST)
    # ì „ ì˜ì—…ì¼ ì°¾ê¸°
    prev_bd = base_day - timedelta(days=1)
    while not is_business_day(prev_bd):
        prev_bd = prev_bd - timedelta(days=1)

    start = datetime(prev_bd.year, prev_bd.month, prev_bd.day, 0, 0, 0, tzinfo=KST)
    return start, end

def in_window(dt: Optional[datetime], start: datetime, end: datetime) -> bool:
    if dt is None:
        return False
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST)
    return start <= dt <= end

def http_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })
    return s


# ==========================
# ë„ë©”ì¸ë³„ "ì§„ì§œ ê¸°ì‚¬ URL" ê²€ì¦
# ==========================
def is_valid_article_url(url: str) -> bool:
    if not url:
        return False

    p = urlparse(url)
    host = (p.netloc or "").lower()
    path = (p.path or "").lower()
    qs = parse_qs(p.query or "")

    # Google / ì™¸ë¶€ ì¤‘ê°„ ë§í¬ ì°¨ë‹¨
    if "news.google.com" in host or "google.com" in host:
        return False

    # Inven: /board/ ëŠ” ë¬´ì¡°ê±´ ê²Œì‹œíŒ
    if host.endswith("inven.co.kr"):
        if "/board/" in path:
            return False
        # webzine newsì¸ë° news íŒŒë¼ë¯¸í„° ì—†ëŠ” ê²€ìƒ‰/ëª©ë¡ì€ ì œì™¸
        if path.startswith("/webzine/news"):
            if "news" not in qs:
                return False
        # keywordë§Œ ìˆëŠ” í˜ì´ì§€ ì œì™¸
        if "keyword" in qs and "news" not in qs:
            return False

    # Gameple: ê¸°ì‚¬ URLì€ /news/articleView.html?idxno= ê°€ ì‚¬ì‹¤ìƒ ì •ë‹µ
    if host.endswith("gameple.co.kr"):
        if "/news/articleview.html" not in path:
            return False
        if "idxno" not in qs:
            return False

    # Gametoc: ê¸°ì‚¬ URLì€ /news/articleView.html?idxno=
    if host.endswith("gametoc.co.kr"):
        if "/news/articleview.html" not in path:
            return False
        if "idxno" not in qs:
            return False

    # Gamemeca: ê¸°ì‚¬ URLì€ /view.php?gid=...
    if host.endswith("gamemeca.com"):
        if "/view.php" not in path:
            return False
        if "gid" not in qs:
            return False

    return True


# ==========================
# ë°ì´í„° êµ¬ì¡°
# ==========================
@dataclass
class Article:
    press: str
    title: str
    url: str
    published: datetime
    snippet: str = ""
    keyword: str = ""

def to_kst(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        return dt.replace(tzinfo=KST)
    return dt.astimezone(KST)


# ==========================
# ìˆ˜ì§‘ê¸° 1) ì¸ë²¤ RSS
# ==========================
def fetch_inven_rss(start: datetime, end: datetime) -> List[Article]:
    s = http_session()
    r = s.get(INVEN_RSS, timeout=TIMEOUT)
    r.raise_for_status()
    feed = feedparser.parse(r.text)

    out: List[Article] = []
    for e in feed.entries[:200]:
        title = clean_text(getattr(e, "title", ""))
        link = clean_text(getattr(e, "link", ""))
        if not title or not link:
            continue

        # RSSì—ì„œ ë“¤ì–´ì˜¤ëŠ” linkê°€ ì¸ë²¤ ê¸°ì‚¬/ë‰´ìŠ¤ê°€ ì•„ë‹ ìˆ˜ë„ ìˆì–´ì„œ URL í•„í„°
        if not is_valid_article_url(link):
            continue

        if looks_like_non_article(title):
            continue

        t = getattr(e, "published_parsed", None) or getattr(e, "updated_parsed", None)
        if not t:
            continue
        pub = datetime(*t[:6], tzinfo=ZoneInfo("UTC")).astimezone(KST)

        if not in_window(pub, start, end):
            continue

        snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
        snippet = clean_text(strip_html(snippet_raw))[:180]

        out.append(Article(
            press="ì¸ë²¤",
            title=title[:140],
            url=link,
            published=pub,
            snippet=snippet,
        ))
    return out


# ==========================
# ìˆ˜ì§‘ê¸° 2) ê²Œì„ë©”ì¹´ ë¦¬ìŠ¤íŠ¸(HTML)
# ==========================
_GAMEMECA_ITEM_RE = re.compile(
    r'href="(?P<href>/view\.php\?gid=\d+)"[^>]*>(?P<title>[^<]+)</a>.*?\n.*?(?P<dt>\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2})',
    re.DOTALL
)

def fetch_gamemeca_list(start: datetime, end: datetime) -> List[Article]:
    s = http_session()
    r = s.get(GAMEMECA_LIST, timeout=TIMEOUT)
    r.raise_for_status()
    html = r.text

    out: List[Article] = []
    for m in _GAMEMECA_ITEM_RE.finditer(html):
        href = m.group("href")
        title = clean_text(m.group("title"))
        dt_str = m.group("dt")

        if not title:
            continue
        if looks_like_non_article(title):
            continue

        url = urljoin("https://www.gamemeca.com", href)
        if not is_valid_article_url(url):
            continue

        try:
            pub = datetime.strptime(dt_str, "%Y.%m.%d %H:%M").replace(tzinfo=KST)
        except Exception:
            continue

        if not in_window(pub, start, end):
            continue

        out.append(Article(
            press="ê²Œì„ë©”ì¹´",
            title=title[:140],
            url=url,
            published=pub,
        ))
    return out


# ==========================
# ìˆ˜ì§‘ê¸° 3) ê²Œì„í”Œ í™ˆ(HTML) â†’ articleView ë§í¬ ì¶”ì¶œ í›„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì…ë ¥ì‹œê°„ íŒŒì‹±
# ==========================
_GAMEPLE_LINK_RE = re.compile(r'href="(?P<href>/news/articleView\.html\?idxno=\d+)"')
_GAMEPLE_TIME_RE = re.compile(r"ì…ë ¥\s*(\d{4}\.\d{2}\.\d{2})\s*(\d{2}:\d{2})")

def fetch_gameple(start: datetime, end: datetime) -> List[Article]:
    s = http_session()
    r = s.get(GAMEPLE_HOME, timeout=TIMEOUT)
    r.raise_for_status()
    html = r.text

    hrefs = list({m.group("href") for m in _GAMEPLE_LINK_RE.finditer(html)})[:60]
    out: List[Article] = []

    for href in hrefs:
        url = urljoin(GAMEPLE_HOME, href)
        if not is_valid_article_url(url):
            continue

        # ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì‹œê°„/ì œëª© í™•ë³´
        try:
            rr = s.get(url, timeout=TIMEOUT)
            rr.raise_for_status()
            art_html = rr.text
        except Exception:
            continue

        # ì œëª©(og:title ìš°ì„ )
        title = ""
        og = re.search(r'<meta[^>]+property="og:title"[^>]+content="([^"]+)"', art_html)
        if og:
            title = clean_text(og.group(1))
        if not title:
            # h íƒœê·¸ fallback
            h = re.search(r"<h\d[^>]*>([^<]+)</h\d>", art_html)
            if h:
                title = clean_text(h.group(1))

        if not title or looks_like_non_article(title):
            continue

        tm = _GAMEPLE_TIME_RE.search(art_html)
        if not tm:
            continue
        dt_str = tm.group(1) + " " + tm.group(2)
        try:
            pub = datetime.strptime(dt_str, "%Y.%m.%d %H:%M").replace(tzinfo=KST)
        except Exception:
            continue

        if not in_window(pub, start, end):
            continue

        out.append(Article(
            press="ê²Œì„í”Œ",
            title=title[:140],
            url=url,
            published=pub,
        ))

        time.sleep(random.uniform(0.05, 0.12))  # ê²½ëŸ‰ ì˜ˆì˜ìƒ ìŠ¬ë¦½
    return out


# ==========================
# ìˆ˜ì§‘ê¸° 4) ê²Œì„í†¡ ë¦¬ìŠ¤íŠ¸(HTML) â†’ articleView ë§í¬ ì¶”ì¶œ + ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì…ë ¥ì‹œê°„ íŒŒì‹±
# (403ì´ ìˆì„ ìˆ˜ ìˆì–´ UA í—¤ë”ë¡œ ì‹œë„)
# ==========================
_GAMETOC_LINK_RE = re.compile(r'href="(?P<href>/news/articleView\.html\?idxno=\d+)"')
_GAMETOC_TIME_RE = re.compile(r"ì…ë ¥\s*(\d{4}\.\d{2}\.\d{2})\s*(\d{2}:\d{2})")

def fetch_gametoc(start: datetime, end: datetime) -> List[Article]:
    s = http_session()
    try:
        r = s.get(GAMETOC_LIST, timeout=TIMEOUT)
        r.raise_for_status()
    except Exception:
        return []

    html = r.text
    hrefs = list({m.group("href") for m in _GAMETOC_LINK_RE.finditer(html)})[:60]
    out: List[Article] = []

    for href in hrefs:
        url = urljoin("https://www.gametoc.co.kr", href)
        if not is_valid_article_url(url):
            continue

        try:
            rr = s.get(url, timeout=TIMEOUT)
            rr.raise_for_status()
            art_html = rr.text
        except Exception:
            continue

        title = ""
        og = re.search(r'<meta[^>]+property="og:title"[^>]+content="([^"]+)"', art_html)
        if og:
            title = clean_text(og.group(1))
        if not title:
            h = re.search(r"<h\d[^>]*>([^<]+)</h\d>", art_html)
            if h:
                title = clean_text(h.group(1))

        if not title or looks_like_non_article(title):
            continue

        tm = _GAMETOC_TIME_RE.search(art_html)
        if not tm:
            continue
        dt_str = tm.group(1) + " " + tm.group(2)
        try:
            pub = datetime.strptime(dt_str, "%Y.%m.%d %H:%M").replace(tzinfo=KST)
        except Exception:
            continue

        if not in_window(pub, start, end):
            continue

        out.append(Article(
            press="ê²Œì„í†¡",
            title=title[:140],
            url=url,
            published=pub,
        ))

        time.sleep(random.uniform(0.05, 0.12))
    return out


# ==========================
# ì§‘ê³„/í•„í„°/ì •ë ¬
# ==========================
def keyword_filter(articles: List[Article], keywords: List[str]) -> List[Article]:
    out = []
    for a in articles:
        blob = (a.title or "")
        if any(k.lower() in blob.lower() for k in keywords):
            out.append(a)
    return out

def dedup(articles: List[Article]) -> List[Article]:
    seen: Dict[str, Article] = {}
    for a in articles:
        sid = stable_id(a.title, a.url)
        seen[sid] = a
    # ìµœì‹ ìˆœ
    return sorted(seen.values(), key=lambda x: x.published, reverse=True)

def build_slack_message(general: List[Article], nexon: List[Article], start: datetime, end: datetime) -> List[str]:
    header = (
        f"## ğŸ“° ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘\n"
        f"- window: {start.strftime('%Y-%m-%d %H:%M')} ~ {end.strftime('%Y-%m-%d %H:%M')} (KST)\n\n"
    )

    def fmt(a: Article) -> str:
        pub = a.published.strftime("%Y-%m-%d %H:%M")
        return f":arrow_forward: [{a.press}] {a.title} ({pub})\n    - <{a.url}|ë§í¬>\n"

    body = "### ğŸŒ ì£¼ìš” ê¸°ì‚¬\n"
    if not general:
        body += "- (ì—†ìŒ)\n"
    else:
        for a in general[:GENERAL_SEND_LIMIT]:
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ê¸°ì‚¬ (ì •í™•ë§¤ì¹­: ì œëª©/ìš”ì•½ì— ë„¥ìŠ¨ í¬í•¨)\n"
    if not nexon:
        body += "- (ì—†ìŒ)\n"
    else:
        for a in nexon[:NEXON_SEND_LIMIT]:
            body += fmt(a)

    full = header + body

    # ìŠ¬ë™ ê¸¸ì´ ë¶„í• 
    msgs: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            msgs.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        msgs.append(chunk)
    return msgs

def send_to_slack(text: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("SLACK_WEBHOOK_URL env is missing")
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps({"text": text}),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()


# ==========================
# Main
# ==========================
def main():
    now = datetime.now(KST)
    start, end = compute_window(now)

    print(f"[INFO] window: {start} ~ {end} (KST)")

    # 1) ì†ŒìŠ¤ë³„ ìˆ˜ì§‘
    collected: List[Article] = []
    stats = {}

    try:
        inv = fetch_inven_rss(start, end)
        stats["inven"] = len(inv)
        collected.extend(inv)
    except Exception as e:
        print(f"[WARN] inven failed: {e}")
        stats["inven"] = 0

    try:
        gm = fetch_gamemeca_list(start, end)
        stats["gamemeca"] = len(gm)
        collected.extend(gm)
    except Exception as e:
        print(f"[WARN] gamemeca failed: {e}")
        stats["gamemeca"] = 0

    try:
        gp = fetch_gameple(start, end)
        stats["gameple"] = len(gp)
        collected.extend(gp)
    except Exception as e:
        print(f"[WARN] gameple failed: {e}")
        stats["gameple"] = 0

    try:
        gt = fetch_gametoc(start, end)
        stats["gametoc"] = len(gt)
        collected.extend(gt)
    except Exception as e:
        print(f"[WARN] gametoc failed: {e}")
        stats["gametoc"] = 0

    # 2) í‚¤ì›Œë“œ í•„í„°(ë„ˆê°€ ì›ë˜ ì›í–ˆë˜ â€œê²Œì„ì—…ê³„ í•µì‹¬ ì´ìŠˆâ€ë§Œ ë‚¨ê¹€)
    filtered = keyword_filter(collected, PRIMARY_KEYWORDS)

    # 3) URL ìµœì¢… ê²€ì¦ + ì¤‘ë³µ ì œê±°
    filtered = [a for a in filtered if is_valid_article_url(a.url)]
    general = dedup(filtered)

    # 4) ë„¥ìŠ¨: â€œí‚¤ì›Œë“œ + ë„¥ìŠ¨(ì •í™•ë§¤ì¹­)â€ êµì§‘í•©
    nexon_candidates = [a for a in general if contains_nexon(a.title, a.snippet)]
    nexon = dedup(nexon_candidates)

    print(f"[INFO] stats: {stats}")
    print(f"[INFO] general={len(general)} nexon={len(nexon)}")
    print("[INFO] preview general:")
    for a in general[:10]:
        print(" -", a.title, "::", a.url)
    print("[INFO] preview nexon:")
    for a in nexon[:10]:
        print(" -", a.title, "::", a.url)

    # 5) Slack ì „ì†¡
    msgs = build_slack_message(general, nexon, start, end)
    for i, m in enumerate(msgs, 1):
        send_to_slack(m)
        print(f"[INFO] sent slack {i}/{len(msgs)}")
        time.sleep(0.2)

if __name__ == "__main__":
    main()
