# ------------------------------------------------------------------
# [ìµœì  ìš´ì˜ìš©] Google News RSS + ë³¸ë¬¸ ìš”ì•½ + Slack ì „ì†¡(ìë™ ë¶„í• ) (2026-02-23)
# - googlesearch-python ì‚¬ìš© ì•ˆ í•¨ (ì°¨ë‹¨/0ê±´ ë¦¬ìŠ¤í¬ ì œê±°)
# - requirements.txt: requests, feedparser, beautifulsoup4, lxml, trafilatura
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs, urlunparse

import requests
import feedparser
from bs4 import BeautifulSoup

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

SEARCH_DAYS = 14
MAX_ITEMS_PER_QUERY = 12

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (NewsDigestBot/1.0; SlackWebhook)"
SLEEP_BETWEEN_REQUESTS = (0.2, 0.6)

SUMMARY_CHARS = 320

# Slack í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì‹¤íŒ¨/ì˜ë¦¼ ìœ„í—˜ â†’ ë³´ìˆ˜ì ìœ¼ë¡œ ë¶„í• 
SLACK_TEXT_LIMIT = 3500


# ==========================
# ìœ í‹¸
# ==========================
def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_REQUESTS))

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _normalize_url(raw_url: str) -> str:
    """
    - google.com/url?q=... í˜•íƒœë©´ qì—ì„œ ì‹¤ URL ë³µì›
    - fragment ì œê±°
    - utm_* ì œê±°
    """
    if not raw_url:
        return raw_url

    # Google redirect URL ì²˜ë¦¬
    try:
        p = urlparse(raw_url)
        if p.netloc in ("www.google.com", "google.com") and p.path == "/url":
            q = parse_qs(p.query).get("q")
            if q and q[0]:
                raw_url = q[0]
    except Exception:
        pass

    # UTM ì œê±° + fragment ì œê±°
    try:
        p = urlparse(raw_url)
        qs = parse_qs(p.query)
        for k in list(qs.keys()):
            if k.lower().startswith("utm_"):
                qs.pop(k, None)

        parts = []
        for k, vs in qs.items():
            for v in vs:
                parts.append(f"{k}={v}")
        new_query = "&".join(parts)

        return urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, ""))
    except Exception:
        return raw_url

def _press_from_url(url: str) -> str:
    try:
        netloc = urlparse(url).netloc.lower().replace("www.", "")
        return netloc.split(".")[0].upper() if netloc else "NEWS"
    except Exception:
        return "NEWS"

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1(f"{title}||{link}".encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> datetime | None:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: datetime | None, days: int) -> bool:
    if not dt:
        # ë‚ ì§œê°€ ì—†ìœ¼ë©´ í¬í•¨(ë„ˆë¬´ ì—„ê²©í•˜ë©´ ê²°ê³¼ê°€ 0 ë  ìˆ˜ ìˆìŒ)
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _google_news_rss_url(keyword: str, site: str, days: int) -> str:
    # when:NdëŠ” ìµœê·¼ Nì¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ëŒì–´ì˜¤ëŠ” í¸
    q = f'"{keyword}" site:{site} when:{days}d'
    return "https://news.google.com/rss/search?q=" + quote(q) + "&hl=ko&gl=KR&ceid=KR:ko"


# ==========================
# ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
# ==========================
def extract_summary(url: str, session: requests.Session) -> str:
    """
    ìš°ì„ : trafilatura (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
    fallback: og/meta description + ì²« ë¬¸ë‹¨ ì¡°í•©
    """
    # 1) trafilatura (optional)
    try:
        import trafilatura  # type: ignore
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=False,
                favor_recall=False,
            )
            text = _clean_text(text)
            if text:
                return _truncate(text, SUMMARY_CHARS)
    except Exception:
        pass

    # 2) BeautifulSoup fallback
    try:
        resp = session.get(url, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "lxml")

        meta = soup.find("meta", attrs={"property": "og:description"}) or soup.find("meta", attrs={"name": "description"})
        desc = _clean_text(meta.get("content")) if meta and meta.get("content") else ""

        paras = []
        for p in soup.find_all("p"):
            t = _clean_text(p.get_text(" ", strip=True))
            if len(t) >= 35:
                paras.append(t)
            if len(" ".join(paras)) >= 900:
                break

        combined = _clean_text(" ".join([desc] + paras))
        return _truncate(combined, SUMMARY_CHARS) if combined else ""
    except Exception:
        return ""


# ==========================
# RSS ìˆ˜ì§‘
# ==========================
def fetch_articles() -> list[dict]:
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    articles = {}  # sid -> article

    for kw in PRIMARY_KEYWORDS:
        for site in TARGET_SITES:
            rss = _google_news_rss_url(kw, site, SEARCH_DAYS)
            try:
                resp = session.get(rss, timeout=REQUEST_TIMEOUT)
                resp.raise_for_status()
                feed = feedparser.parse(resp.text)

                got = 0
                for entry in feed.entries:
                    if got >= MAX_ITEMS_PER_QUERY:
                        break

                    title = _clean_text(getattr(entry, "title", ""))
                    link = _normalize_url(_clean_text(getattr(entry, "link", "")))
                    published_dt = _parse_published(entry)

                    if not title or not link:
                        continue
                    # ì•ˆì „ë§: ì§€ì • ë„ë©”ì¸ë§Œ
                    if site not in link:
                        continue
                    # ë‚ ì§œ ì•ˆì „ë§
                    if not _within_days(published_dt, SEARCH_DAYS):
                        continue

                    sid = _stable_id(title, link)
                    if sid in articles:
                        continue

                    articles[sid] = {
                        "keyword": kw,
                        "press": _press_from_url(link),
                        "title": title,
                        "link": link,
                        "published_dt": published_dt,
                        "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                        "summary": "",
                    }
                    got += 1

                _sleep()
            except Exception as e:
                print(f"[WARN] RSS ì‹¤íŒ¨ (kw={kw}, site={site}): {e}")
                continue

    # ìš”ì•½ ì±„ìš°ê¸°
    for a in articles.values():
        a["summary"] = extract_summary(a["link"], session)
        _sleep()

    # ìµœì‹ ìˆœ ì •ë ¬ (published_dt ì—†ëŠ” ê±´ ë’¤ë¡œ)
    def sort_key(x: dict):
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(articles.values(), key=sort_key, reverse=True)


# ==========================
# Slack ë©”ì‹œì§€ ìƒì„±/ì „ì†¡
# ==========================
def is_nexon(article: dict) -> bool:
    blob = f"{article.get('title','')} {article.get('summary','')} {article.get('link','')}".lower()
    return ("ë„¥ìŠ¨" in blob) or ("nexon" in blob)

def build_messages(articles: list[dict]) -> list[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {SEARCH_DAYS}ì¼, Google News RSS)\n"
    header += f"- ëŒ€ìƒ ì‚¬ì´íŠ¸: {', '.join(TARGET_SITES)}\n"
    header += f"- í‚¤ì›Œë“œ: {', '.join(PRIMARY_KEYWORDS)}\n\n"

    def fmt(a: dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        summ = f"\n    - {_truncate(a.get('summary',''), 500)}" if a.get("summary") else ""
        return f"â–¶ *[{a['press']}]* <{a['link']}|{a['title']}>{pub}{summ}\n"

    major = articles
    nexon = [a for a in articles if is_nexon(a)]

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not major:
        body += f"- ìµœê·¼ {SEARCH_DAYS}ì¼ê°„, ì§€ì • ì¡°ê±´ì˜ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in major:
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤\n"
    if not nexon:
        body += "- 'ë„¥ìŠ¨' ê´€ë ¨ ê¸°ì‚¬(ì œëª©/ìš”ì•½/URL ê¸°ì¤€)ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon:
            body += fmt(a)

    full = header + body

    # Slack ê¸¸ì´ ì œí•œ ëŒ€ì‘: ë¼ì¸ ë‹¨ìœ„ ë¶„í• 
    messages = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str):
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

def main():
    articles = fetch_articles()
    print(f"[INFO] fetched articles: {len(articles)}")
    messages = build_messages(articles)

    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.5)

if __name__ == "__main__":
    main()
