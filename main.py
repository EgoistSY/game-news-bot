# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.1] FAST Google News RSS -> Slack Digest (Noise-reduced)
# - Python 3.9 í˜¸í™˜
# - í•µì‹¬ ê°œì„ :
#   1) ì¿¼ë¦¬ì— ê²Œì„ ì»¨í…ìŠ¤íŠ¸ ê°•ì œ (ë…¸ì´ì¦ˆ ëŒ€í­ ê°ì†Œ)
#   2) zdnet/ddailyëŠ” ì¶”ê°€ë¡œ ì—„ê²© í•„í„° (ì œëª©/ìš”ì•½ì— ê²Œì„ íŒíŠ¸ í•„ìš”)
#   3) snippet HTML ì œê±° (Slackì— <a href=...> ì„ì´ëŠ” ë¬¸ì œ ë°©ì§€)
#   4) í´ë°±ì—ì„œë„ ê²Œì„ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ (ì€í–‰/ìœ í†µ/ì¸ì‚¬ ê¸°ì‚¬ ë°©ì§€)
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote

import requests
import feedparser

SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

SEARCH_DAYS = 14

KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18
MAX_ENTRIES_PER_FEED = 30

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.1; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.15)

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 20

# --------------------------
# ê²Œì„ ì»¨í…ìŠ¤íŠ¸ (ì¿¼ë¦¬ ì¡°ì„)
# --------------------------
GAME_CONTEXT_OR = [
    "ê²Œì„", "ê²Œì´ë°", "ê²Œì„ì—…ê³„", "ê²Œì„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œì„", "PCê²Œì„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA",
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

# --------------------------
# â€œê²Œì„ ë§¤ì²´ê°€ ì•„ë‹Œâ€ ì‚¬ì´íŠ¸ëŠ” ë” ì—„ê²©í•˜ê²Œ
# --------------------------
STRICT_SITES = {"zdnet.co.kr", "ddaily.co.kr"}

# ì œëª©/ìš”ì•½ì— ì´ íŒíŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´(íŠ¹íˆ zdnet/ddaily) ë²„ë¦¼
GAME_HINTS = [
    "ê²Œì„", "ê²Œì´ë°", "ì‹ ì‘", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ", "ìŠ¤íŒ€", "ì½˜ì†”", "ëª¨ë°”ì¼", "PC",
    "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox", "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports",
    "ë„¥ìŠ¨", "ì—”ì”¨", "í¬ë˜í”„í†¤", "ë„·ë§ˆë¸”", "ì¹´ì¹´ì˜¤ê²Œì„", "ìŠ¤ë§ˆì¼ê²Œì´íŠ¸", "í„ì–´ë¹„ìŠ¤",
]

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    # snippetì— <a ...> ê°™ì€ ê²Œ ì„ì´ëŠ” ë¬¸ì œ ë°©ì§€
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess(entry) -> str:
    # entry.source.titleì´ ì¢…ì¢… "ê²Œì„ë©”ì¹´" ë“±ìœ¼ë¡œ ë“¤ì–´ì˜´
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _build_query(keyword: str, sites: List[str], days: int, game_context: str) -> str:
    # sitesê°€ ë¹„ì–´ìˆìœ¼ë©´ site ì¡°ê±´ ì—†ì´(í´ë°±) game_context + keywordë§Œ ìœ ì§€
    if sites:
        return f"{game_context} {keyword} {_site_or_query(sites)} when:{days}d"
    return f"{game_context} {keyword} when:{days}d"

def _has_game_hint(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    for h in GAME_HINTS:
        if h.lower() in blob:
            return True
    return False

def fetch_fast(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "strict_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = _build_query(kw, sites, days, GAME_CONTEXT_QUERY)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _clean_text(_strip_html(snippet_raw))
                snippet = _truncate(snippet, SNIPPET_MAX)

                # âœ… ì‚¬ì´íŠ¸ë³„ ì—„ê²© í•„í„°: zdnet/ddailyëŠ” ê²Œì„ íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì œê±°
                # (linkê°€ news.google ì¤‘ê°„ë§í¬ì—¬ë„, title/summaryë¡œ ì¶©ë¶„íˆ ê±°ë¥¼ ìˆ˜ ìˆìŒ)
                if any(s in link for s in STRICT_SITES) or any(s in title for s in ("ì§€ë””ë„·", "ë””ì§€í„¸ë°ì¼ë¦¬")):
                    if not _has_game_hint(title, snippet):
                        stats["strict_filtered_out"] += 1
                        continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()

        except Exception as ex:
            print(f"[WARN] RSS call failed (kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

def _is_nexon(a: Dict) -> bool:
    blob = f"{a.get('title','')} {a.get('snippet','')} {a.get('link','')}".lower()
    return ("ë„¥ìŠ¨" in blob) or ("nexon" in blob)

def build_messages(articles: List[Dict], stats: Dict[str, int], days: int) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ğŸ“° {today_str} ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {days}ì¼)\n"
    header += f"- ìˆ˜ì§‘: feeds={stats.get('feeds_called',0)}, entries={stats.get('entries_seen',0)}, added={stats.get('added',0)}, strict_drop={stats.get('strict_filtered_out',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    major = articles
    nexon = [a for a in articles if _is_nexon(a)]

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not major:
        body += f"- ìµœê·¼ {days}ì¼ ê¸°ì¤€ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in major[:80]:
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤\n"
    if not nexon:
        body += "- 'ë„¥ìŠ¨' ê´€ë ¨ ê¸°ì‚¬(ì œëª©/ìš”ì•½/URL ê¸°ì¤€)ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon[:50]:
            body += fmt(a)

    full = header + body

    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

def main() -> None:
    # 1ì°¨: ìƒìœ„ í‚¤ì›Œë“œë¡œ ì‚¬ì´íŠ¸ ì œí•œ ê²€ìƒ‰
    primary = PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY]
    articles, stats = fetch_fast(primary, TARGET_SITES, SEARCH_DAYS)

    # 0ê±´ì´ë©´: í‚¤ì›Œë“œ í™•ì¥
    if not articles:
        print("[INFO] primary fetch returned 0. fallback to full keyword set.")
        articles, stats = fetch_fast(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, SEARCH_DAYS)

    # ê·¸ë˜ë„ 0ê±´ì´ë©´: ìµœí›„ í´ë°±(ì‚¬ì´íŠ¸ ì¡°ê±´ ì œê±°) BUT ê²Œì„ ì»¨í…ìŠ¤íŠ¸ëŠ” ìœ ì§€
    if not articles:
        print("[INFO] still 0. final fallback: remove site filters (keep game context).")
        articles, stats = fetch_fast(PRIMARY_KEYWORDS[:10], [], SEARCH_DAYS)

    print(f"[INFO] fetched articles: {len(articles)}")
    print(f"[INFO] stats: {stats}")
    print("[INFO] preview:")
    for i, a in enumerate(articles[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    messages = build_messages(articles, stats, SEARCH_DAYS)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.2)

if __name__ == "__main__":
    main()
