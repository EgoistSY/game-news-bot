# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.4] ì •í™• ë§í¬ + ê¸°ì‚¬ ì•„ë‹Œ ê²Œì‹œë¬¼ ì œê±° (ê²½ëŸ‰)
# - Python 3.9 í˜¸í™˜
# - ëª©í‘œ:
#   1) Slackì— ë³´ë‚´ëŠ” ë§í¬ëŠ” "ì›ë¬¸ URL"ë§Œ (Google ì¤‘ê°„ ë§í¬ ì œê±°)
#   2) Inven board/ ê³µëµ/ ê¸¸ë“œëª¨ì§‘/ keyword ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì œê±°
#   3) ë§¤ì¼ KST 10ì‹œì— "ì „ë‚ " ê¸°ì‚¬ë§Œ ë°œì†¡ë˜ë„ë¡ after/before ë‚ ì§œ ë²”ìœ„ ê³ ì •
#
# í•µì‹¬:
# - RSS ìˆ˜ì§‘ì€ ë¹ ë¥´ê²Œ (feedparser)
# - ì›ë¬¸ URLì€ "ë³´ë‚¼ ê¸°ì‚¬ ìƒìœ„ Nê°œë§Œ" ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì œ(HEAD/GET stream) -> ê²½ëŸ‰ ìœ ì§€
# - ë³¸ë¬¸ í¬ë¡¤ë§/íŒŒì‹± ì—†ìŒ
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs

import requests
import feedparser
from zoneinfo import ZoneInfo

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

# ì „ë‚  ë²”ìœ„ë¡œ ê³ ì • (KST)
KST = ZoneInfo("Asia/Seoul")

REQUEST_TIMEOUT = 10
USER_AGENT = "Mozilla/5.0 (GameNewsBot/1.4; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.12)

MAX_ENTRIES_PER_FEED = 30
MAX_ENTRIES_PER_NEXON_FEED = 20

# Slack ì¶œë ¥ ìƒí•œ(ë„ˆë¬´ ë§ìœ¼ë©´ ë…¸ì´ì¦ˆ)
GENERAL_SEND_LIMIT = 50
NEXON_SEND_LIMIT = 5

# ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì œëŠ” "ë³´ë‚¼ ê¸°ì‚¬" + ë°±í•„ìš© ì¼ë¶€ë§Œ ìˆ˜í–‰ (ê²½ëŸ‰)
RESOLVE_BUDGET_GENERAL = 80   # ì¼ë°˜ ê¸°ì‚¬ URL í•´ì œ ìµœëŒ€ ê°œìˆ˜
RESOLVE_BUDGET_NEXON = 30     # ë„¥ìŠ¨ URL í•´ì œ ìµœëŒ€ ê°œìˆ˜

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 12

# ==========================
# ì»¨í…ìŠ¤íŠ¸/í•„í„°
# ==========================
GAME_CONTEXT_OR = [
    "ê²Œì„", "ê²Œì´ë°", "ê²Œì„ì—…ê³„", "ê²Œì„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œì„", "PCê²Œì„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports"
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

# ê²Œì‹œê¸€/ì»¤ë®¤ë‹ˆí‹°ì„±(ê³µëµ/ê¸¸ë“œëª¨ì§‘ ë“±) 1ì°¨ ì»·: ì œëª©/ìš”ì•½ ê¸°ë°˜
NON_ARTICLE_TITLE_HINTS = [
    "ê³µëµ", "íŒ", "ë…¸í•˜ìš°", "ì§ˆë¬¸", "Q&A", "ì¸ì¦", "í›„ê¸°", "ìŠ¤ìƒ·", "ìŠ¤í¬ë¦°ìƒ·",
    "ê¸¸ë“œ", "ê¸¸ë“œëª¨ì§‘", "ëª¨ì§‘", "íŒŒí‹°", "íŒŸ", "ê³ ì •íŒŸ", "í´ëœ", "í´ëœëª¨ì§‘",
    "ê±°ë˜", "ë‚˜ëˆ”", "íŒë§¤", "ì‚½ë‹ˆë‹¤",
    "ë²„ê·¸ì œë³´", "ê±´ì˜", "í† ë¡ ",
]

# Inven URL íŒ¨í„´ í•„í„°(ì›ë¬¸ URL ê¸°ì¤€ìœ¼ë¡œë§Œ ì ìš©)
def is_valid_article_url(url: str) -> bool:
    if not url:
        return False
    try:
        p = urlparse(url)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        qs = parse_qs(p.query or "")
    except Exception:
        return True  # íŒŒì‹± ì‹¤íŒ¨ëŠ” ê³¼í•˜ê²Œ ë²„ë¦¬ê¸°ë³´ë‹¤ í†µê³¼

    # ì¸ë²¤: boardëŠ” ê¸°ì‚¬ ì•„ë‹˜, keyword ë¦¬ìŠ¤íŠ¸ë„ ê¸°ì‚¬ ì•„ë‹˜
    if host.endswith("inven.co.kr"):
        if "/board/" in path:
            return False
        if path.startswith("/webzine/news") or path.startswith("/webzine/news/"):
            # ê¸°ì‚¬ë©´ news=ê°€ ìˆì–´ì•¼ í•¨
            if "news" not in qs:
                return False
        # news ì—†ì´ keywordë§Œ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì œê±°
        if "keyword" in qs and "news" not in qs:
            return False

    return True

# ë„¥ìŠ¨ ê²€ì¦(ì œëª©/ìš”ì•½ì— ë„¥ìŠ¨ì´ ì‹¤ì œ í¬í•¨ë˜ì–´ì•¼ë§Œ ë„¥ìŠ¨ ì„¹ì…˜ì— í¬í•¨)
NEXON_TERMS = [
    "ë„¥ìŠ¨", "nexon",
    "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œì„ì¦ˆ", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤", "ë„¤ì˜¤í”Œ"
]

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

# ë„¥ìŠ¨ ì¤‘ìš”ë„(ê°€ë²¼ìš´ ì ìˆ˜)
NEXON_IMPORTANCE = [
    ("M&A", 5), ("ì¸ìˆ˜", 5), ("í•©ë³‘", 5),
    ("íˆ¬ì", 4), ("ì§€ë¶„", 4),
    ("ì†Œì†¡", 5), ("ê·œì œ", 4),
    ("ë§¤ì¶œ", 4), ("ì‹¤ì ", 4), ("ì˜ì—…ì´ìµ", 4), ("ìˆœì´ìµ", 4),
    ("ì¶œì‹œ", 3), ("ì—…ë°ì´íŠ¸", 3),
    ("ë¦¬ìŠ¤í¬", 3), ("ì•…ì¬", 3), ("í˜¸ì¬", 3),
]

def nexon_score(a: Dict) -> int:
    blob = f"{a.get('title','')} {a.get('snippet','')}".lower()
    score = 0
    for kw, w in NEXON_IMPORTANCE:
        if kw.lower() in blob:
            score += w
    if contains_nexon(a.get("title",""), a.get("snippet","")):
        score += 2
    return score

# ==========================
# ìœ í‹¸
# ==========================
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _press_guess(entry) -> str:
    # Google News RSSëŠ” source.titleë¡œ ì–¸ë¡ ì‚¬ ì´ë¦„ì´ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ(í‘œì‹œìš©)
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _yesterday_range_kst() -> Tuple[str, str, str]:
    now = datetime.now(KST)
    today = now.date()
    yday = today - timedelta(days=1)
    # Google search operator: after/beforeëŠ” ë‚ ì§œ ë¬¸ìì—´ ì‚¬ìš©
    return yday.isoformat(), yday.isoformat(), today.isoformat()

def _looks_like_non_article(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(h.lower() in blob for h in NON_ARTICLE_TITLE_HINTS)

# ==========================
# í•µì‹¬: Google ì¤‘ê°„ ë§í¬ -> ì›ë¬¸ URL í•´ì œ (ê²½ëŸ‰)
# - HEAD ì‹œë„ -> ë§‰íˆë©´ GET stream (ë³¸ë¬¸ ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
# - ìºì‹œë¡œ ì¤‘ë³µ í•´ì œ ë°©ì§€
# ==========================
class UrlResolver:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": USER_AGENT,
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
        })
        self.cache: Dict[str, str] = {}

    def resolve(self, url: str) -> str:
        if not url:
            return url
        if url in self.cache:
            return self.cache[url]

        final_url = url
        try:
            # 1) HEAD allow_redirects
            r = self.session.head(url, allow_redirects=True, timeout=REQUEST_TIMEOUT)
            if r.url:
                final_url = r.url
        except Exception:
            # 2) fallback: GET stream (ë³¸ë¬¸ ë‹¤ìš´ë¡œë“œ ì—†ì´)
            try:
                r = self.session.get(url, allow_redirects=True, timeout=REQUEST_TIMEOUT, stream=True)
                if r.url:
                    final_url = r.url
                try:
                    r.close()
                except Exception:
                    pass
            except Exception:
                final_url = url

        self.cache[url] = final_url
        return final_url

# ==========================
# ì¿¼ë¦¬ ë¹Œë”
# ==========================
def build_query_general(keyword: str, sites: List[str], after: str, before: str) -> str:
    # ì „ë‚  ê³ ì • after/before
    return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} after:{after} before:{before}"

def build_query_nexon(keyword: str, sites: List[str], after: str, before: str) -> str:
    # ë„¥ìŠ¨ì€ í‚¤ì›Œë“œ+ë„¥ìŠ¨ êµì§‘í•©
    nexon_expr = '("ë„¥ìŠ¨" OR Nexon OR "ë„¥ìŠ¨ê²Œì„ì¦ˆ" OR ë„¤ì˜¤í”Œ)'
    return f'{nexon_expr} {keyword} {_site_or_query(sites)} after:{after} before:{before}'

# ==========================
# RSS ìˆ˜ì§‘ (ë§í¬ëŠ” ì•„ì§ google link ìƒíƒœë¡œ ì €ì¥)
# ==========================
def fetch_track(track: str,
                keywords: List[str],
                max_entries_per_feed: int,
                query_builder,
                after: str,
                before: str) -> Tuple[List[Dict], Dict[str, int]]:

    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "non_article_hint_drop": 0,
        "added": 0,
    }

    found: Dict[str, Dict] = {}

    for kw in keywords:
        q = query_builder(kw, TARGET_SITES, after, before)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:max_entries_per_feed]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                if not title:
                    continue

                google_link = _clean_text(getattr(e, "link", "") or "")
                if not google_link:
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                # 1ì°¨: ì œëª©/ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ ê³µëµ/ê¸¸ë“œëª¨ì§‘ ë“± ì»· (ë§¤ìš° ê°€ë²¼ì›€)
                if _looks_like_non_article(title, snippet):
                    stats["non_article_hint_drop"] += 1
                    continue

                published_dt = _parse_published(e)
                press = _press_guess(e)

                sid = _stable_id(title, google_link)
                if sid in found:
                    continue

                found[sid] = {
                    "track": track,
                    "keyword": kw,
                    "press": press,
                    "title": _truncate(title, TITLE_MAX),
                    "google_link": google_link,   # ì¤‘ê°„ ë§í¬
                    "link": google_link,          # ìµœì¢…ì ìœ¼ë¡œ ì›ë¬¸ìœ¼ë¡œ êµì²´í•  ì˜ˆì •
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed ({track} kw={kw}): {ex}")
            continue

    # ìµœì‹ ìˆœ ê¸°ë³¸ ì •ë ¬
    def sort_key(a: Dict) -> datetime:
        return a["published_dt"] if a.get("published_dt") else datetime.min

    items = sorted(list(found.values()), key=sort_key, reverse=True)
    return items, stats

# ==========================
# ë§í¬ í•´ì œ + URL íŒ¨í„´ í•„í„° ì ìš© + ë°±í•„
# ==========================
def finalize_links_and_filter(items: List[Dict], resolver: UrlResolver, budget: int) -> Tuple[List[Dict], Dict[str, int]]:
    stats = {
        "resolved": 0,
        "url_pattern_drop": 0,
        "resolve_failed_or_google_left": 0,
    }

    out: List[Dict] = []
    used = 0

    for a in items:
        if used >= budget:
            # ì˜ˆì‚° ë„˜ìœ¼ë©´ google ë§í¬ ê·¸ëŒ€ë¡œ(ë‹¨, ì¸ë²¤ ê²Œì‹œíŒ ê°™ì€ ê±¸ ëª» ê±¸ëŸ¬ì„œ ìœ„í—˜)
            # -> ì˜ˆì‚°ì€ SEND_LIMITë³´ë‹¤ ì—¬ìœ  ìˆê²Œ ì¡ì•„ë‘ (ìœ„ì—ì„œ 80/30)
            a["link"] = a["google_link"]
            stats["resolve_failed_or_google_left"] += 1
            out.append(a)
            continue

        g = a.get("google_link", "")
        final = resolver.resolve(g)
        used += 1
        stats["resolved"] += 1

        a["link"] = final

        # ì›ë¬¸ URL íŒ¨í„´ìœ¼ë¡œ "ê¸°ì‚¬ ì•„ë‹Œ ë§í¬" í™•ì‹¤í•˜ê²Œ ì œê±°
        if not is_valid_article_url(final):
            stats["url_pattern_drop"] += 1
            continue

        out.append(a)

    # ì¤‘ë³µ(ì›ë¬¸ URL ê¸°ì¤€)
    dedup: Dict[str, Dict] = {}
    for a in out:
        sid = _stable_id(a.get("title",""), a.get("link",""))
        dedup[sid] = a

    # ë‹¤ì‹œ ìµœì‹ ìˆœ ì •ë ¬
    def sort_key(a: Dict) -> datetime:
        return a["published_dt"] if a.get("published_dt") else datetime.min

    return sorted(list(dedup.values()), key=sort_key, reverse=True), stats

# ==========================
# Slack ë©”ì‹œì§€
# ==========================
def build_messages(general: List[Dict], nexon: List[Dict],
                   stats: Dict[str, Dict], yday_label: str) -> List[str]:
    header = f"## ğŸ“° {yday_label} ì „ë‚  ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ë°œì†¡: KST 10:00)\n"
    header += f"- general: feeds={stats['general_rss']['feeds_called']}, entries={stats['general_rss']['entries_seen']}, rss_added={stats['general_rss']['added']}, hint_drop={stats['general_rss']['non_article_hint_drop']}, resolved={stats['general_finalize']['resolved']}, url_drop={stats['general_finalize']['url_pattern_drop']}\n"
    header += f"- nexon: feeds={stats['nexon_rss']['feeds_called']}, entries={stats['nexon_rss']['entries_seen']}, rss_added={stats['nexon_rss']['added']}, hint_drop={stats['nexon_rss']['non_article_hint_drop']}, resolved={stats['nexon_finalize']['resolved']}, url_drop={stats['nexon_finalize']['url_pattern_drop']}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not general:
        body += "- ì „ë‚  ê¸°ì¤€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in general[:GENERAL_SEND_LIMIT]:
            body += fmt(a)

    # ë„¥ìŠ¨: ì •í™•ë„ ê°•ì œ(ì œëª©/ìš”ì•½ ë„¥ìŠ¨ í¬í•¨) + ì¤‘ìš”ë„ Top 5
    nexon_true = [a for a in nexon if contains_nexon(a.get("title",""), a.get("snippet",""))]
    nexon_sorted = sorted(nexon_true, key=lambda x: (nexon_score(x), x["published_dt"] or datetime.min), reverse=True)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ (Top 5)\n"
    if not nexon_sorted:
        body += "- ì „ë‚  ê¸°ì¤€ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon_sorted[:NEXON_SEND_LIMIT]:
            body += fmt(a)

    full = header + body

    # Slack ê¸¸ì´ ë¶„í• 
    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

# ==========================
# Main
# ==========================
def main() -> None:
    yday_label, after, before = _yesterday_range_kst()

    # 1) RSSë¡œ í›„ë³´ ìˆ˜ì§‘
    general_keywords = PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY]
    general_items, stats_general_rss = fetch_track(
        track="general",
        keywords=general_keywords,
        max_entries_per_feed=MAX_ENTRIES_PER_FEED,
        query_builder=build_query_general,
        after=after,
        before=before,
    )
    # ë„ˆë¬´ ì ìœ¼ë©´ í‚¤ì›Œë“œ í™•ì¥
    if len(general_items) < 10:
        general_items, stats_general_rss = fetch_track(
            track="general",
            keywords=PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK],
            max_entries_per_feed=MAX_ENTRIES_PER_FEED,
            query_builder=build_query_general,
            after=after,
            before=before,
        )

    nexon_items, stats_nexon_rss = fetch_track(
        track="nexon",
        keywords=PRIMARY_KEYWORDS,
        max_entries_per_feed=MAX_ENTRIES_PER_NEXON_FEED,
        query_builder=build_query_nexon,
        after=after,
        before=before,
    )

    # 2) "ë³´ë‚¼ ë§Œí¼ë§Œ" ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì œí•´ì„œ ì›ë¬¸ URL í™•ì • + URL íŒ¨í„´ í•„í„°
    resolver = UrlResolver()
    general_final, stats_general_finalize = finalize_links_and_filter(general_items, resolver, RESOLVE_BUDGET_GENERAL)
    nexon_final, stats_nexon_finalize = finalize_links_and_filter(nexon_items, resolver, RESOLVE_BUDGET_NEXON)

    # 3) ë””ë²„ê·¸ ë¯¸ë¦¬ë³´ê¸°(ì•¡ì…˜ ë¡œê·¸ìš©)
    print(f"[INFO] KST range: after={after} before={before} (yday={yday_label})")
    print(f"[INFO] general rss={len(general_items)} final={len(general_final)}")
    print(f"[INFO] nexon rss={len(nexon_items)} final={len(nexon_final)}")

    print("[INFO] preview general:")
    for i, a in enumerate(general_final[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. {a.get('title','')} :: {a.get('link','')}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon_final[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. {a.get('title','')} :: {a.get('link','')}")

    stats = {
        "general_rss": stats_general_rss,
        "nexon_rss": stats_nexon_rss,
        "general_finalize": stats_general_finalize,
        "nexon_finalize": stats_nexon_finalize,
    }

    # 4) Slack ì „ì†¡
    messages = build_messages(general_final, nexon_final, stats, yday_label)
    for idx, msg in enumerate(messages, 1):
        send_to_slack(msg)
        print(f"[INFO] sent slack {idx}/{len(messages)}")
        time.sleep(0.15)

if __name__ == "__main__":
    main()
