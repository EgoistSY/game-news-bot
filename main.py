# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.3.1] FAST Google News RSS -> Slack Digest (1-day + Nexon precision + non-article URL filter)
# - Python 3.9 í˜¸í™˜
# - ì¶”ê°€: "ê¸°ì‚¬ ì•„ë‹Œ ë§í¬" URL íŒ¨í„´ í•„í„°(ì´ˆê²½ëŸ‰)
#   * inven.co.kr/board/ ì œì™¸
#   * inven.co.kr/webzine/news/?keyword= ë“± news id ì—†ëŠ” ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ ì œì™¸
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs

import requests
import feedparser

SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ìž‘", "ì„±ê³¼", "í˜¸ìž¬", "ì•…ìž¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ìž", "M&A"
]

SEARCH_DAYS = 1

KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18
MAX_ENTRIES_PER_FEED = 30
MAX_ENTRIES_PER_NEXON_FEED = 20

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.31; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.12)

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 15

# --------------------------
# ì»¨í…ìŠ¤íŠ¸/í•„í„°
# --------------------------
GAME_CONTEXT_OR = [
    "ê²Œìž„", "ê²Œì´ë°", "ê²Œìž„ì—…ê³„", "ê²Œìž„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œìž„", "PCê²Œìž„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports"
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

STRICT_SITES = {"zdnet.co.kr", "ddaily.co.kr"}

GAME_HINTS = [
    "ê²Œìž„", "ê²Œì´ë°", "ì‹ ìž‘", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ", "ìŠ¤íŒ€", "ì½˜ì†”", "ëª¨ë°”ì¼", "PC",
    "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox", "RPG", "MMORPG", "FPS", "MOBA",
    "eìŠ¤í¬ì¸ ", "esports",
    "ë„¥ìŠ¨", "ì—”ì”¨", "í¬ëž˜í”„í†¤", "ë„·ë§ˆë¸”", "ì¹´ì¹´ì˜¤ê²Œìž„", "ìŠ¤ë§ˆì¼ê²Œì´íŠ¸", "íŽ„ì–´ë¹„ìŠ¤",
]

NEXON_TERMS = [
    "ë„¥ìŠ¨", "nexon",
    "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œìž„ì¦ˆ", "ë„¥ìŠ¨ ë„¤íŠ¸ì›ìŠ¤", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤",
    "ë„¤ì˜¤í”Œ", "ë„¥ìŠ¨GT", "ë„¥ìŠ¨ì§€í‹°",
]

NEXON_IMPORTANCE = [
    ("M&A", 5), ("ì¸ìˆ˜", 5), ("í•©ë³‘", 5),
    ("íˆ¬ìž", 4), ("ì§€ë¶„", 4),
    ("ì†Œì†¡", 5), ("ê·œì œ", 4),
    ("ë§¤ì¶œ", 4), ("ì‹¤ì ", 4), ("ì˜ì—…ì´ìµ", 4), ("ìˆœì´ìµ", 4),
    ("ì¶œì‹œ", 3), ("ì—…ë°ì´íŠ¸", 3),
    ("CBT", 2), ("OBT", 2),
    ("ë¦¬ìŠ¤í¬", 3), ("ì•…ìž¬", 3), ("í˜¸ìž¬", 3),
]

# --------------------------
# URL ìœ íš¨ì„±(ê¸°ì‚¬ ì—¬ë¶€) í•„í„°: "ì´ˆê²½ëŸ‰"
# --------------------------
def is_valid_article_url(url: str) -> bool:
    """
    'ê¸°ì‚¬'ê°€ ì•„ë‹Œ íŽ˜ì´ì§€(ê²Œì‹œíŒ, ê²€ìƒ‰/ë¦¬ìŠ¤íŠ¸, íƒœê·¸, ëž­í‚¹ ë“±)ë¥¼ URL íŒ¨í„´ìœ¼ë¡œ ì œê±°.
    - ë¬´ê±°ìš´ ë³¸ë¬¸ í¬ë¡¤ë§ ì—†ì´ ì²˜ë¦¬(ìš´ì˜ ì†ë„ ìœ ì§€).
    """
    if not url:
        return False

    try:
        p = urlparse(url)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        qs = parse_qs(p.query or "")
    except Exception:
        return True  # íŒŒì‹± ì‹¤íŒ¨ëŠ” ê³¼í•˜ê²Œ ë²„ë¦¬ì§€ ì•Šê¸° ìœ„í•´ í†µê³¼

    # ê³µí†µ: ë³´ë“œ/í¬ëŸ¼ë¥˜ ê²½ë¡œ ì œê±°
    # (ì¶”ê°€ ì‚¬ì´íŠ¸ê°€ ìƒê¸°ë©´ ì—¬ê¸°ì— ê³„ì† ì–¹ìœ¼ë©´ ë¨)
    common_bad_path_tokens = [
        "/board/",        # ì¸ë²¤ ê²Œì‹œíŒ
        "/search",        # ê²€ìƒ‰ íŽ˜ì´ì§€
        "/tag/",          # íƒœê·¸ ëª©ë¡
        "/rank",          # ëž­í‚¹
        "/ranking",
        "/gallery",       # ê°¤ëŸ¬ë¦¬(ê¸°ì‚¬ì™€ ë¬´ê´€í•  ê°€ëŠ¥ì„± ë†’ìŒ)
    ]
    if any(tok in path for tok in common_bad_path_tokens):
        return False

    # Inven íŠ¹í™”
    if host.endswith("inven.co.kr"):
        # ê²Œì‹œíŒì€ ë¬´ì¡°ê±´ ì œì™¸
        if "/board/" in path:
            return False

        # webzine/news ëŠ” "news=ê¸°ì‚¬ID"ê°€ ìžˆì–´ì•¼ ê¸°ì‚¬ë¡œ ê°„ì£¼
        if path.startswith("/webzine/news/") or path == "/webzine/news":
            if "news" not in qs:
                # keyword= ê°™ì€ ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ ì°¨ë‹¨
                return False

        # ê·¸ ì™¸ ì¸ë²¤ ë„ë©”ì¸ì—ì„œ ê¸°ì‚¬ë¡œ ë³´ê¸° ì• ë§¤í•œ ë£¨íŠ¸ë“¤ë„ ì œì™¸(í•„ìš”ì‹œ í™•ìž¥)
        if path in ("/", "/webzine", "/webzine/"):
            return False

    return True

# --------------------------
# ìœ í‹¸
# --------------------------
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess(entry) -> str:
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _has_any_hint(text: str, hints: List[str]) -> bool:
    blob = (text or "").lower()
    return any(h.lower() in blob for h in hints)

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

def nexon_score(article: Dict) -> int:
    blob = f"{article.get('title','')} {article.get('snippet','')}".lower()
    score = 0
    for kw, w in NEXON_IMPORTANCE:
        if kw.lower() in blob:
            score += w
    if contains_nexon(article.get("title", ""), article.get("snippet", "")):
        score += 2
    return score

# --------------------------
# ì¿¼ë¦¬
# --------------------------
def build_query_general(keyword: str, sites: List[str], days: int) -> str:
    if sites:
        return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} when:{days}d"
    return f"{GAME_CONTEXT_QUERY} {keyword} when:{days}d"

def build_query_nexon(keyword: str, sites: List[str], days: int) -> str:
    nexon_expr = '("ë„¥ìŠ¨" OR Nexon OR "ë„¥ìŠ¨ê²Œìž„ì¦ˆ" OR ë„¤ì˜¤í”Œ)'
    if sites:
        return f'{nexon_expr} {keyword} {_site_or_query(sites)} when:{days}d'
    return f'{nexon_expr} {keyword} when:{days}d'

# --------------------------
# RSS ìˆ˜ì§‘
# --------------------------
def fetch_general(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "strict_filtered_out": 0,
        "non_article_url_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_general(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                # âœ… ê¸°ì‚¬ URL íŒ¨í„´ í•„í„°
                if not is_valid_article_url(link):
                    stats["non_article_url_filtered_out"] += 1
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                # ì¼ë°˜ íŠ¸ëž™: zdnet/ddailyëŠ” ê²Œìž„ ížŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì œê±°
                if any(s in link for s in STRICT_SITES) or any(s in title for s in ("ì§€ë””ë„·", "ë””ì§€í„¸ë°ì¼ë¦¬")):
                    if not _has_any_hint(f"{title} {snippet}", GAME_HINTS):
                        stats["strict_filtered_out"] += 1
                        continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "general",
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (general kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

def fetch_nexon(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "nexon_filtered_out": 0,
        "non_article_url_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = build_query_nexon(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_NEXON_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                # âœ… ê¸°ì‚¬ URL íŒ¨í„´ í•„í„°
                if not is_valid_article_url(link):
                    stats["non_article_url_filtered_out"] += 1
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                # âœ… ë„¥ìŠ¨ ìµœì¢… ê²€ì¦(ì œëª©/ìš”ì•½ì— ì‹¤ì œ ë„¥ìŠ¨ í¬í•¨)
                if not contains_nexon(title, snippet):
                    stats["nexon_filtered_out"] += 1
                    continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "nexon",
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed (nexon kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

# --------------------------
# Slack ë©”ì‹œì§€
# --------------------------
def build_messages(general: List[Dict], nexon: List[Dict],
                   stats_g: Dict[str, int], stats_n: Dict[str, int], days: int) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## ðŸ“° {today_str} ê²Œìž„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ (ìµœê·¼ {days}ì¼)\n"
    header += f"- general: feeds={stats_g.get('feeds_called',0)}, entries={stats_g.get('entries_seen',0)}, added={stats_g.get('added',0)}, strict_drop={stats_g.get('strict_filtered_out',0)}, non_article_drop={stats_g.get('non_article_url_filtered_out',0)}\n"
    header += f"- nexon: feeds={stats_n.get('feeds_called',0)}, entries={stats_n.get('entries_seen',0)}, added={stats_n.get('added',0)}, nexon_drop={stats_n.get('nexon_filtered_out',0)}, non_article_drop={stats_n.get('non_article_url_filtered_out',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    body = "### ðŸŒ ì£¼ìš” ê²Œìž„ì—…ê³„ ë‰´ìŠ¤\n"
    if not general:
        body += "- ì˜¤ëŠ˜ ê¸°ì¤€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in general[:70]:
            body += fmt(a)

    body += "\n---\n### ðŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ (Top 5)\n"
    if not nexon:
        body += "- ë„¥ìŠ¨ ê´€ë ¨ ë‰´ìŠ¤(í‚¤ì›Œë“œ êµì§‘í•© + ì œëª©/ìš”ì•½ ê²€ì¦)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
    else:
        scored = sorted(nexon, key=lambda x: (nexon_score(x), x["published_dt"] or datetime.min), reverse=True)
        for a in scored[:5]:
            body += fmt(a)

    full = header + body

    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

# --------------------------
# Main
# --------------------------
def main() -> None:
    general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY], TARGET_SITES, SEARCH_DAYS)
    if not general:
        general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, SEARCH_DAYS)

    nexon, stats_n = fetch_nexon(PRIMARY_KEYWORDS, TARGET_SITES, SEARCH_DAYS)

    print(f"[INFO] general fetched: {len(general)}, stats: {stats_g}")
    print(f"[INFO] nexon fetched: {len(nexon)}, stats: {stats_n}")
    print("[INFO] preview general:")
    for i, a in enumerate(general[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    messages = build_messages(general, nexon, stats_g, stats_n, SEARCH_DAYS)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.15)

if __name__ == "__main__":
    main()
