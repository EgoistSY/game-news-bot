# ------------------------------------------------------------------
# [Ïö¥ÏòÅÏö© v4.2] FAST Google News RSS -> Slack Digest (1-day + Nexon dedicated track)
# - Python 3.9 Ìò∏Ìôò
# - Î≥ÄÍ≤ΩÏ†ê:
#   1) SEARCH_DAYS=1 (ÌïòÎ£®Ïπò)
#   2) ÎÑ•Ïä® Ï†ÑÏö© Ìä∏Îûô: Nexon Í¥ÄÎ†® ÏøºÎ¶¨Î•º Î≥ÑÎèÑÎ°ú ÏàòÌñâÌïòÏó¨ Ï§ëÏöî Í∏∞ÏÇ¨ ÎàÑÎùΩ Î∞©ÏßÄ
#   3) ÎÑ•Ïä® Ìä∏ÎûôÏùÄ ÌïÑÌÑ∞Î•º ÏôÑÌôîÌïòÍ≥† Í≤∞Í≥º ÏÉÅÌïúÏùÑ ÎÇÆÏ∂∞ "Ï†ïÌôïÎèÑ+Ïª§Î≤ÑÎ¶¨ÏßÄ" Í∑†Ìòï
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from urllib.parse import quote

import requests
import feedparser

SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "Ïã†Ïûë", "ÏÑ±Í≥º", "Ìò∏Ïû¨", "ÏïÖÏû¨", "Î¶¨Ïä§ÌÅ¨", "Ï†ïÏ±Ö", "ÏóÖÎç∞Ïù¥Ìä∏", "Ï∂úÏãú",
    "Îß§Ï∂ú", "ÏàúÏúÑ", "ÏÜåÏÜ°", "Í∑úÏ†ú", "CBT", "OBT", "Ïù∏Ïàò", "Ìà¨Ïûê", "M&A"
]

# ‚úÖ ÌïòÎ£®ÏπòÎßå
SEARCH_DAYS = 1

KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18
MAX_ENTRIES_PER_FEED = 30

# ÎÑ•Ïä® Ï†ÑÏö© Ìä∏ÎûôÏùÄ Ìò∏Ï∂ú Ïàò/Í≤∞Í≥º ÏàòÎ•º Ï†úÌïú (ÏÜçÎèÑ/ÌíàÏßà)
NEXON_QUERIES = [
    "ÎÑ•Ïä®",
    "Nexon",
    # ÌïÑÏöîÌïòÎ©¥ ÏïÑÎûòÏ≤òÎüº Í≤åÏûÑ/Ïù¥ÏäàÎ•º Ï∂îÍ∞ÄÌï¥ÎèÑ Ï¢ãÏùå(ÎÑàÍ∞Ä Ïã§Ï†úÎ°ú Ï§ëÏöîÌïòÍ≤å Î≥¥Îäî Ï∂ï)
    # "Î©îÏù¥ÌîåÏä§ÌÜ†Î¶¨", "ÎçòÏ†ÑÏï§ÌååÏù¥ÌÑ∞", "FC Ïò®ÎùºÏù∏", "Î∏îÎ£® ÏïÑÏπ¥Ïù¥Î∏å"
]
MAX_ENTRIES_PER_NEXON_FEED = 25  # ÎÑ•Ïä® Ï†ÑÏö©ÏùÄ Ï∂©Î∂ÑÌûà

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (FastNewsDigestBot/1.2; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.15)

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 20

# --------------------------
# Í≤åÏûÑ Ïª®ÌÖçÏä§Ìä∏ (ÏùºÎ∞ò Ìä∏Îûô ÎÖ∏Ïù¥Ï¶à ÏñµÏ†ú)
# --------------------------
GAME_CONTEXT_OR = [
    "Í≤åÏûÑ", "Í≤åÏù¥Î∞ç", "Í≤åÏûÑÏóÖÍ≥Ñ", "Í≤åÏûÑÏÇ¨", "ÌçºÎ∏îÎ¶¨ÏÖî", "Í∞úÎ∞úÏÇ¨",
    "Î™®Î∞îÏùºÍ≤åÏûÑ", "PCÍ≤åÏûÑ", "ÏΩòÏÜî", "Ïä§ÌåÄ", "Steam", "PS5", "ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò", "ÎãåÌÖêÎèÑ", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA",
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

STRICT_SITES = {"zdnet.co.kr", "ddaily.co.kr"}

GAME_HINTS = [
    "Í≤åÏûÑ", "Í≤åÏù¥Î∞ç", "Ïã†Ïûë", "ÏóÖÎç∞Ïù¥Ìä∏", "Ï∂úÏãú", "Ïä§ÌåÄ", "ÏΩòÏÜî", "Î™®Î∞îÏùº", "PC",
    "ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò", "ÎãåÌÖêÎèÑ", "Xbox", "RPG", "MMORPG", "FPS", "MOBA", "eÏä§Ìè¨Ï∏†", "esports",
    "ÎÑ•Ïä®", "ÏóîÏî®", "ÌÅ¨ÎûòÌîÑÌÜ§", "ÎÑ∑ÎßàÎ∏î", "Ïπ¥Ïπ¥Ïò§Í≤åÏûÑ", "Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏", "ÌéÑÏñ¥ÎπÑÏä§",
]

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "‚Ä¶"

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _parse_published(entry) -> Optional[datetime]:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    try:
        return datetime(*t[:6])
    except Exception:
        return None

def _within_days(dt: Optional[datetime], days: int) -> bool:
    if not dt:
        return True
    return dt >= (datetime.now() - timedelta(days=days))

def _press_guess(entry) -> str:
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _build_query_general(keyword: str, sites: List[str], days: int) -> str:
    # ÏùºÎ∞ò Ìä∏Îûô: Í≤åÏûÑ Ïª®ÌÖçÏä§Ìä∏ + ÌÇ§ÏõåÎìú + ÏÇ¨Ïù¥Ìä∏ + when
    if sites:
        return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} when:{days}d"
    return f"{GAME_CONTEXT_QUERY} {keyword} when:{days}d"

def _build_query_nexon(nexon_term: str, sites: List[str], days: int) -> str:
    # ÎÑ•Ïä® Ìä∏Îûô: ÎÑ•Ïä® ÏûêÏ≤¥Í∞Ä Í∞ïÌïú ÏãúÍ∑∏ÎÑêÏù¥ÎØÄÎ°ú Í≤åÏûÑ Ïª®ÌÖçÏä§Ìä∏Î•º Í∞ïÏ†úÌïòÏßÄ ÏïäÏùå(ÎàÑÎùΩ Î∞©ÏßÄ)
    # ÎåÄÏã† ÏÇ¨Ïù¥Ìä∏ Ï†úÌïúÏùÄ Ïú†ÏßÄ
    if sites:
        return f'{nexon_term} {_site_or_query(sites)} when:{days}d'
    return f'{nexon_term} when:{days}d'

def _has_game_hint(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    for h in GAME_HINTS:
        if h.lower() in blob:
            return True
    return False

def fetch_general(keywords: List[str], sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "strict_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for kw in keywords:
        q = _build_query_general(kw, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _clean_text(_strip_html(snippet_raw))
                snippet = _truncate(snippet, SNIPPET_MAX)

                # zdnet/ddaily Ï∂îÍ∞Ä ÏóÑÍ≤© ÌïÑÌÑ∞ (ÏùºÎ∞ò Ìä∏ÎûôÎßå Ï†ÅÏö©)
                if any(s in link for s in STRICT_SITES) or any(s in title for s in ("ÏßÄÎîîÎÑ∑", "ÎîîÏßÄÌÑ∏Îç∞ÏùºÎ¶¨")):
                    if not _has_game_hint(title, snippet):
                        stats["strict_filtered_out"] += 1
                        continue

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "general",
                    "keyword": kw,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()

        except Exception as ex:
            print(f"[WARN] RSS call failed (general kw={kw}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

def fetch_nexon(sites: List[str], days: int) -> Tuple[List[Dict], Dict[str, int]]:
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "date_filtered_out": 0,
        "added": 0,
    }

    articles: Dict[str, Dict] = {}

    for term in NEXON_QUERIES:
        q = _build_query_nexon(term, sites, days)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:MAX_ENTRIES_PER_NEXON_FEED]:
                stats["entries_seen"] += 1

                title = _clean_text(getattr(e, "title", ""))
                link = _clean_text(getattr(e, "link", ""))
                if not title or not link:
                    continue

                published_dt = _parse_published(e)
                if not _within_days(published_dt, days):
                    stats["date_filtered_out"] += 1
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _clean_text(_strip_html(snippet_raw))
                snippet = _truncate(snippet, SNIPPET_MAX)

                sid = _stable_id(title, link)
                if sid in articles:
                    continue

                articles[sid] = {
                    "track": "nexon",
                    "keyword": term,
                    "press": _press_guess(e),
                    "title": _truncate(title, TITLE_MAX),
                    "link": link,
                    "published_dt": published_dt,
                    "published": published_dt.strftime("%Y-%m-%d %H:%M") if published_dt else "",
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()

        except Exception as ex:
            print(f"[WARN] RSS call failed (nexon term={term}): {ex}")
            continue

    def sort_key(x: Dict) -> datetime:
        return x["published_dt"] if x.get("published_dt") else datetime.min

    return sorted(list(articles.values()), key=sort_key, reverse=True), stats

def _is_nexon(a: Dict) -> bool:
    blob = f"{a.get('title','')} {a.get('snippet','')} {a.get('link','')}".lower()
    return ("ÎÑ•Ïä®" in blob) or ("nexon" in blob)

def build_messages(general_articles: List[Dict], nexon_articles: List[Dict],
                   stats_general: Dict[str, int], stats_nexon: Dict[str, int], days: int) -> List[str]:
    today_str = datetime.now().strftime("%Y-%m-%d")
    header = f"## üì∞ {today_str} Í≤åÏûÑÏóÖÍ≥Ñ Îâ¥Ïä§ Î∏åÎ¶¨Ìïë (ÏµúÍ∑º {days}Ïùº)\n"
    header += f"- general: feeds={stats_general.get('feeds_called',0)}, entries={stats_general.get('entries_seen',0)}, added={stats_general.get('added',0)}, strict_drop={stats_general.get('strict_filtered_out',0)}\n"
    header += f"- nexon: feeds={stats_nexon.get('feeds_called',0)}, entries={stats_nexon.get('entries_seen',0)}, added={stats_nexon.get('added',0)}\n\n"

    def fmt(a: Dict) -> str:
        pub = f" ({a['published']})" if a.get("published") else ""
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"‚ñ∂ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}>{pub}{sn}\n"

    # ÏùºÎ∞ò Îâ¥Ïä§ (ÏÉÅÌïú Ïú†ÏßÄ)
    body = "### üåê Ï£ºÏöî Í≤åÏûÑÏóÖÍ≥Ñ Îâ¥Ïä§\n"
    if not general_articles:
        body += f"- ÏµúÍ∑º {days}Ïùº Í∏∞Ï§Ä Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.\n"
    else:
        for a in general_articles[:80]:
            body += fmt(a)

    # ÎÑ•Ïä® Îâ¥Ïä§Îäî ‚ÄúÏ†ÑÏö© Ìä∏Îûô Í≤∞Í≥º + (ÏùºÎ∞ò Ìä∏ÎûôÏóêÏÑú ÎÑ•Ïä®ÏúºÎ°ú Í±∏Î¶∞ Í≤É)‚Äù Ìï©Ï≥êÏÑú Ï§ëÎ≥µ Ï†úÍ±∞
    merged = {}
    for a in nexon_articles:
        merged[_stable_id(a["title"], a["link"])] = a
    for a in general_articles:
        if _is_nexon(a):
            merged[_stable_id(a["title"], a["link"])] = a

    merged_list = list(merged.values())
    merged_list.sort(key=lambda x: x["published_dt"] if x.get("published_dt") else datetime.min, reverse=True)

    body += "\n---\n### üè¢ ÎÑ•Ïä® Í¥ÄÎ†® Ï£ºÏöî Îâ¥Ïä§\n"
    if not merged_list:
        body += "- 'ÎÑ•Ïä®' Í¥ÄÎ†® Í∏∞ÏÇ¨(Ï†úÎ™©/ÏöîÏïΩ/URL Í∏∞Ï§Ä)Í∞Ä ÏóÜÏäµÎãàÎã§.\n"
    else:
        for a in merged_list[:30]:
            body += fmt(a)

    full = header + body

    # Slack Í∏∏Ïù¥ Ï†úÌïú ÎåÄÏùë
    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > 3500:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)

    return messages

def send_to_slack_text(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("ÌôòÍ≤ΩÎ≥ÄÏàò SLACK_WEBHOOK_URLÏù¥ ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏßÄ ÏïäÏäµÎãàÎã§.")

    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

def main() -> None:
    # ÏùºÎ∞ò Ìä∏Îûô
    primary = PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY]
    general, stats_g = fetch_general(primary, TARGET_SITES, SEARCH_DAYS)
    if not general:
        general, stats_g = fetch_general(PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], TARGET_SITES, SEARCH_DAYS)

    # ÎÑ•Ïä® Ìä∏Îûô (Î≥ÑÎèÑ)
    nexon, stats_n = fetch_nexon(TARGET_SITES, SEARCH_DAYS)

    print(f"[INFO] general fetched: {len(general)}, stats: {stats_g}")
    print(f"[INFO] nexon fetched: {len(nexon)}, stats: {stats_n}")

    print("[INFO] preview general:")
    for i, a in enumerate(general[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. [{a.get('press','NEWS')}] {a.get('title','')} :: {a.get('link','')}")

    messages = build_messages(general, nexon, stats_g, stats_n, SEARCH_DAYS)
    for i, msg in enumerate(messages, 1):
        send_to_slack_text(msg)
        print(f"[INFO] sent slack message {i}/{len(messages)}")
        time.sleep(0.2)

if __name__ == "__main__":
    main()
