# ------------------------------------------------------------------
# [ìš´ì˜ìš© v4.6] ì›ë¬¸ URL 100% ì§€í–¥ + ê¸°ì‚¬ ì•„ë‹Œ ë§í¬ ì œê±° + KST ìœˆë„ìš°(ì£¼ë§/ê³µíœ´ì¼ ëˆ„ì )
# - Python 3.9 í˜¸í™˜
# - ëª©í‘œ:
#   1) Slack ë§í¬ëŠ” "ì›ë¬¸ URL"ë§Œ (news.google.com ë§í¬ëŠ” ë°œì†¡í•˜ì§€ ì•ŠìŒ)
#   2) Inven board/ ê³µëµ/ ê¸¸ë“œëª¨ì§‘/ keyword ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì œê±°
#   3) ë§¤ì¼ KST 10ì‹œì— "ì „ë‚ (ì£¼ë§/ê³µíœ´ì¼ ëˆ„ì )" ìœˆë„ìš° ê¸°ì‚¬ ë°œì†¡
#
# í•µì‹¬ ìˆ˜ì •:
# - Google ì¤‘ê°„ ë§í¬( news.google.com/rss/articles/... )ë¥¼ ì—´ì–´ HTMLì—ì„œ ì›ë¬¸ URLì„ ì¶”ì¶œ
# - HTMLì„ ë„ˆë¬´ ì¡°ê¸ˆë§Œ ì½ì–´ì„œ ì‹¤íŒ¨í•˜ë˜ ë¬¸ì œ í•´ê²°: ìµœëŒ€ 512KBê¹Œì§€ ì½ìœ¼ë©° URL íƒìƒ‰
# - ì›ë¬¸ URLì´ ì•ˆ ë‚˜ì˜¤ë©´ "ë²„ë¦¼"(êµ¬ê¸€ ë§í¬ ë°œì†¡ ê¸ˆì§€)
# ------------------------------------------------------------------
import os
import re
import json
import time
import hashlib
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, time as dtime, timezone
from urllib.parse import quote, urlparse, parse_qs, unquote
from email.utils import parsedate_to_datetime

import requests
import feedparser
from zoneinfo import ZoneInfo

# holidaysëŠ” ì„ íƒ(ì—†ìœ¼ë©´ ì£¼ë§ë§Œ)
try:
    import holidays as holidays_lib  # pip install holidays
except Exception:
    holidays_lib = None

# ==========================
# ì„¤ì •
# ==========================
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TARGET_SITES = [
    "inven.co.kr",
    "gamemeca.com",
    "thisisgame.com",
    "gametoc.co.kr",
    "gameple.co.kr",
    "zdnet.co.kr",
    "ddaily.co.kr",
]

PRIMARY_KEYWORDS = [
    "ì‹ ì‘", "ì„±ê³¼", "í˜¸ì¬", "ì•…ì¬", "ë¦¬ìŠ¤í¬", "ì •ì±…", "ì—…ë°ì´íŠ¸", "ì¶œì‹œ",
    "ë§¤ì¶œ", "ìˆœìœ„", "ì†Œì†¡", "ê·œì œ", "CBT", "OBT", "ì¸ìˆ˜", "íˆ¬ì", "M&A"
]

KEYWORD_BATCH_PRIMARY = 10
KEYWORD_BATCH_FALLBACK = 18

KST = ZoneInfo("Asia/Seoul")
SEND_HOUR = 10
END_CUTOFF = dtime(9, 59, 59)

REQUEST_TIMEOUT = 12
USER_AGENT = "Mozilla/5.0 (GameNewsBot/1.60; SlackWebhook)"
SLEEP_BETWEEN_FEEDS = (0.05, 0.12)

MAX_ENTRIES_PER_FEED = 30
MAX_ENTRIES_PER_NEXON_FEED = 20

GENERAL_SEND_LIMIT = 50
NEXON_SEND_LIMIT = 5

# ì›ë¬¸ í•´ì œëŠ” í›„ë³´ë¥¼ ë„‰ë„‰íˆ(í•˜ì§€ë§Œ ê³¼ë„í•˜ê²Œ ë¬´ê²ì§€ ì•Šê²Œ)
RESOLVE_BUDGET_GENERAL = 120
RESOLVE_BUDGET_NEXON = 80

SLACK_TEXT_LIMIT = 3500
TITLE_MAX = 120
SNIPPET_MAX = 180
PREVIEW_TOP_N = 12

# Google ì¤‘ê°„ ë§í¬ HTMLì—ì„œ ì›ë¬¸ URL ì°¾ì„ ë•Œ ì½ì„ ìµœëŒ€ ë°”ì´íŠ¸
MAX_HTML_BYTES = 512 * 1024  # 512KB

# ==========================
# ì»¨í…ìŠ¤íŠ¸/í•„í„°
# ==========================
GAME_CONTEXT_OR = [
    "ê²Œì„", "ê²Œì´ë°", "ê²Œì„ì—…ê³„", "ê²Œì„ì‚¬", "í¼ë¸”ë¦¬ì…”", "ê°œë°œì‚¬",
    "ëª¨ë°”ì¼ê²Œì„", "PCê²Œì„", "ì½˜ì†”", "ìŠ¤íŒ€", "Steam", "PS5", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜", "ë‹Œí…ë„", "Xbox",
    "RPG", "MMORPG", "FPS", "MOBA", "eìŠ¤í¬ì¸ ", "esports"
]
GAME_CONTEXT_QUERY = "(" + " OR ".join(GAME_CONTEXT_OR) + ")"

NON_ARTICLE_TITLE_HINTS = [
    "ê³µëµ", "íŒ", "ë…¸í•˜ìš°", "ì§ˆë¬¸", "q&a", "Q&A", "ì¸ì¦", "í›„ê¸°", "ìŠ¤ìƒ·", "ìŠ¤í¬ë¦°ìƒ·",
    "ê¸¸ë“œ", "ê¸¸ë“œëª¨ì§‘", "ê¸¸ë“œ ëª¨ì§‘", "ëª¨ì§‘", "í´ëœ", "í´ëœëª¨ì§‘",
    "íŒŒí‹°", "íŒŸ", "ê³ ì •íŒŸ",
    "ê±°ë˜", "ë‚˜ëˆ”", "íŒë§¤", "ì‚½ë‹ˆë‹¤",
    "ë²„ê·¸ì œë³´", "ê±´ì˜", "í† ë¡ ",
]

NEXON_TERMS = ["ë„¥ìŠ¨", "nexon", "ë„¥ìŠ¨ì½”ë¦¬ì•„", "ë„¥ìŠ¨ê²Œì„ì¦ˆ", "ë„¥ìŠ¨ë„¤íŠ¸ì›ìŠ¤", "ë„¤ì˜¤í”Œ"]

NEXON_IMPORTANCE = [
    ("M&A", 5), ("ì¸ìˆ˜", 5), ("í•©ë³‘", 5),
    ("íˆ¬ì", 4), ("ì§€ë¶„", 4),
    ("ì†Œì†¡", 5), ("ê·œì œ", 4),
    ("ë§¤ì¶œ", 4), ("ì‹¤ì ", 4), ("ì˜ì—…ì´ìµ", 4), ("ìˆœì´ìµ", 4),
    ("ì¶œì‹œ", 3), ("ì—…ë°ì´íŠ¸", 3),
    ("ë¦¬ìŠ¤í¬", 3), ("ì•…ì¬", 3), ("í˜¸ì¬", 3),
]

_GOOGLE_HOSTS = {"news.google.com", "www.google.com", "google.com"}

# ==========================
# ìœ í‹¸
# ==========================
def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[: max(0, n - 1)] + "â€¦"

def _stable_id(title: str, link: str) -> str:
    return hashlib.sha1((title + "||" + link).encode("utf-8")).hexdigest()[:16]

def _sleep():
    time.sleep(random.uniform(*SLEEP_BETWEEN_FEEDS))

def _google_news_rss_search_url(query: str) -> str:
    return "https://news.google.com/rss/search?q=" + quote(query) + "&hl=ko&gl=KR&ceid=KR:ko"

def _site_or_query(sites: List[str]) -> str:
    return "(" + " OR ".join([f"site:{s}" for s in sites]) + ")"

def _press_guess(entry) -> str:
    try:
        src = getattr(entry, "source", None)
        if src and isinstance(src, dict):
            t = _clean_text(src.get("title", ""))
            if t:
                return t
    except Exception:
        pass
    return "NEWS"

def _looks_like_non_article(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(h.lower() in blob for h in NON_ARTICLE_TITLE_HINTS)

def _is_google_url(url: str) -> bool:
    try:
        h = (urlparse(url).netloc or "").lower()
        return h in _GOOGLE_HOSTS or h.endswith(".google.com")
    except Exception:
        return True

def contains_nexon(title: str, snippet: str) -> bool:
    blob = f"{title} {snippet}".lower()
    return any(t.lower() in blob for t in NEXON_TERMS)

def nexon_score(a: Dict) -> int:
    blob = f"{a.get('title','')} {a.get('snippet','')}".lower()
    score = 0
    for kw, w in NEXON_IMPORTANCE:
        if kw.lower() in blob:
            score += w
    if contains_nexon(a.get("title",""), a.get("snippet","")):
        score += 2
    return score

# ==========================
# URL íŒ¨í„´ í•„í„°(ì›ë¬¸ URL ê¸°ì¤€)
# ==========================
def is_valid_article_url(url: str) -> bool:
    if not url:
        return False
    try:
        p = urlparse(url)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        qs = parse_qs(p.query or "")
    except Exception:
        return False

    if path in ("", "/"):
        return False

    if host.endswith("inven.co.kr"):
        # boardëŠ” ê¸°ì‚¬ ì•„ë‹˜
        if "/board/" in path:
            return False
        # webzine/news ëŠ” news= ìˆì–´ì•¼ ê¸°ì‚¬
        if path.startswith("/webzine/news"):
            if "news" not in qs:
                return False
        # news ì—†ì´ keywordë§Œ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ëŠ” ì œê±°
        if "keyword" in qs and "news" not in qs:
            return False

    return True

# ==========================
# ë‚ ì§œ íŒŒì‹±(KST aware)
# ==========================
def parse_entry_datetime_kst(entry) -> Optional[datetime]:
    for attr in ("published", "updated"):
        s = getattr(entry, attr, None)
        if s:
            try:
                dt = parsedate_to_datetime(s)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.astimezone(KST)
            except Exception:
                pass

    for attr in ("published_parsed", "updated_parsed"):
        t = getattr(entry, attr, None)
        if t:
            try:
                naive = datetime(*t[:6])
                return naive.replace(tzinfo=timezone.utc).astimezone(KST)
            except Exception:
                pass
    return None

# ==========================
# ìœˆë„ìš° ê³„ì‚°(ì£¼ë§/ê³µíœ´ì¼ ëˆ„ì )
# - ì˜ˆ: ì›”ìš”ì¼ì´ë©´ ê¸ˆìš”ì¼ 00:00 ~ ì›” 09:59
# ==========================
def compute_window_kst(now_kst: datetime) -> Tuple[datetime, datetime, str]:
    end_dt = datetime.combine(now_kst.date(), END_CUTOFF, tzinfo=KST)

    kr_holidays = None
    if holidays_lib is not None:
        try:
            kr_holidays = holidays_lib.country_holidays("KR", years=[now_kst.year, now_kst.year - 1])
        except Exception:
            kr_holidays = None

    if kr_holidays is None:
        print("[WARN] holidays ë¯¸ì„¤ì¹˜/ì˜¤ë¥˜ë¡œ ê³µíœ´ì¼ì€ ì œì™¸í•˜ì§€ ì•Šê³  ì£¼ë§ë§Œ ëˆ„ì  ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    def is_business_day(d) -> bool:
        if d.weekday() >= 5:
            return False
        if kr_holidays is not None and d in kr_holidays:
            return False
        return True

    d = now_kst.date() - timedelta(days=1)
    while not is_business_day(d):
        d -= timedelta(days=1)
    prev_business = d

    start_dt = datetime.combine(prev_business, dtime(SEND_HOUR, 0, 0), tzinfo=KST)

    # ì›”ìš”ì¼ + ì§ì „ ì˜ì—…ì¼ì´ ê¸ˆìš”ì¼ì´ë©´ ê¸ˆìš”ì¼ 00:00ë¶€í„°
    if now_kst.weekday() == 0 and prev_business.weekday() == 4:
        start_dt = datetime.combine(prev_business, dtime(0, 0, 0), tzinfo=KST)

    label = f"{start_dt.strftime('%Y-%m-%d %H:%M')} ~ {end_dt.strftime('%Y-%m-%d %H:%M')} (KST)"
    return start_dt, end_dt, label

# ==========================
# ì¿¼ë¦¬ ë¹Œë”
# ==========================
def build_query_general(keyword: str, sites: List[str], after_date: str, before_date: str) -> str:
    return f"{GAME_CONTEXT_QUERY} {keyword} {_site_or_query(sites)} after:{after_date} before:{before_date}"

def build_query_nexon(keyword: str, sites: List[str], after_date: str, before_date: str) -> str:
    nexon_expr = '("ë„¥ìŠ¨" OR Nexon OR "ë„¥ìŠ¨ê²Œì„ì¦ˆ" OR ë„¤ì˜¤í”Œ)'
    return f'{nexon_expr} {keyword} {_site_or_query(sites)} after:{after_date} before:{before_date}'

# ==========================
# í•µì‹¬: Google ì¤‘ê°„ ë§í¬ HTMLì—ì„œ ì›ë¬¸ URL ì¶”ì¶œ
# ==========================
_URL_RE = re.compile(r"https?://[^\s\"'<>]+", re.IGNORECASE)

def _collect_target_urls(text: str) -> List[str]:
    urls = _URL_RE.findall(text or "")
    out = []
    for u in urls:
        u = u.split("&amp;")[0]
        try:
            u = unquote(u)
        except Exception:
            pass
        if any(site in u for site in TARGET_SITES):
            out.append(u)
    return out

def _score_candidate(u: str) -> int:
    """ì›ë¬¸ URL í›„ë³´ ì ìˆ˜(ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)."""
    sc = 0
    try:
        p = urlparse(u)
        host = (p.netloc or "").lower()
        path = (p.path or "")
        qs = parse_qs(p.query or "")
    except Exception:
        return -999

    # target siteì´ë©´ ê¸°ë³¸ ê°€ì‚°
    if any(s in u for s in TARGET_SITES):
        sc += 10

    # ë„ˆë¬´ ì§§ì€ ê²½ë¡œ(í™ˆ/ì„¹ì…˜) ê°ì 
    if path in ("", "/"):
        sc -= 50
    if len(path) < 6:
        sc -= 10

    # ì¸ë²¤ì€ ê¸°ì‚¬(news=) ê°•í•œ ê°€ì‚°, board ê°•í•œ ê°ì 
    if "inven.co.kr" in host:
        if "/board/" in (path or "").lower():
            sc -= 80
        if "/webzine/news" in (path or "").lower() and "news" in qs:
            sc += 60
        if "keyword" in qs and "news" not in qs:
            sc -= 60

    return sc

class PublisherResolver:
    """news.google.com/rss/articles/... ë¥¼ ì—´ì–´ì„œ ì›ë¬¸ URLì„ ìµœëŒ€í•œ ë½‘ì•„ë‚¸ë‹¤."""
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": USER_AGENT,
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
        })
        self.cache: Dict[str, Optional[str]] = {}

    def resolve(self, google_url: str) -> Optional[str]:
        if not google_url:
            return None
        if google_url in self.cache:
            return self.cache[google_url]

        # 1) ë‹¨ìˆœ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¡œ ë°”ë¡œ ì›ë¬¸ì´ ë‚˜ì˜¤ë©´ ë² ìŠ¤íŠ¸
        try:
            r = self.session.get(google_url, allow_redirects=True, timeout=REQUEST_TIMEOUT)
            final = r.url or ""
            if final and (not _is_google_url(final)) and any(s in final for s in TARGET_SITES):
                if is_valid_article_url(final):
                    self.cache[google_url] = final
                    return final
        except Exception:
            pass

        # 2) HTMLì„ ì¼ë¶€ ì½ì–´ ì›ë¬¸ í›„ë³´ URL ì¶”ì¶œ(ì°¨ë‹¨/ë™ì˜í˜ì´ì§€ì—¬ë„ URLì´ ë°•í˜€ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ)
        try:
            r = self.session.get(google_url, allow_redirects=True, timeout=REQUEST_TIMEOUT, stream=True)
            buf = bytearray()
            for chunk in r.iter_content(chunk_size=16384):
                if not chunk:
                    break
                buf.extend(chunk)
                if len(buf) >= MAX_HTML_BYTES:
                    break
            try:
                r.close()
            except Exception:
                pass

            text = buf.decode("utf-8", errors="ignore")
            cands = _collect_target_urls(text)
            if cands:
                cands = list(dict.fromkeys(cands))  # stable unique
                cands.sort(key=_score_candidate, reverse=True)

                # ì ìˆ˜ ë†’ì€ ê²ƒë¶€í„° ê¸°ì‚¬ URL í•„í„° í†µê³¼í•˜ëŠ” ê²ƒ ì„ íƒ
                for u in cands[:25]:
                    if is_valid_article_url(u):
                        self.cache[google_url] = u
                        return u
        except Exception:
            pass

        # ì‹¤íŒ¨
        self.cache[google_url] = None
        return None

# ==========================
# RSS ìˆ˜ì§‘
# ==========================
def fetch_track(track: str,
                keywords: List[str],
                max_entries_per_feed: int,
                query_builder,
                after_date: str,
                before_date: str,
                start_dt: datetime,
                end_dt: datetime) -> Tuple[List[Dict], Dict[str, int]]:

    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.6",
    })

    stats = {
        "feeds_called": 0,
        "entries_seen": 0,
        "hint_drop": 0,
        "no_date_drop": 0,
        "too_old_drop": 0,
        "window_drop": 0,
        "added": 0,
    }

    found: Dict[str, Dict] = {}
    hard_old_cutoff = start_dt - timedelta(days=1)  # 2010 ê°™ì€ ì´ìƒì¹˜ ë°©ì§€

    for kw in keywords:
        q = query_builder(kw, TARGET_SITES, after_date, before_date)
        url = _google_news_rss_search_url(q)

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            stats["feeds_called"] += 1

            feed = feedparser.parse(resp.text)
            for e in feed.entries[:max_entries_per_feed]:
                stats["entries_seen"] += 1

                title = _truncate(_clean_text(getattr(e, "title", "")), TITLE_MAX)
                if not title:
                    continue

                google_link = _clean_text(getattr(e, "link", "") or "")
                if not google_link:
                    continue

                snippet_raw = getattr(e, "summary", "") or getattr(e, "description", "") or ""
                snippet = _truncate(_clean_text(_strip_html(snippet_raw)), SNIPPET_MAX)

                if _looks_like_non_article(title, snippet):
                    stats["hint_drop"] += 1
                    continue

                pub_kst = parse_entry_datetime_kst(e)
                if not pub_kst:
                    stats["no_date_drop"] += 1
                    continue

                if pub_kst < hard_old_cutoff:
                    stats["too_old_drop"] += 1
                    continue

                if pub_kst < start_dt or pub_kst > end_dt:
                    stats["window_drop"] += 1
                    continue

                press = _press_guess(e)
                sid = _stable_id(title, google_link)
                if sid in found:
                    continue

                found[sid] = {
                    "track": track,
                    "keyword": kw,
                    "press": press,
                    "title": title,
                    "google_link": google_link,   # google ì¤‘ê°„
                    "link": None,                # ì›ë¬¸ìœ¼ë¡œ í™•ì •ë  ê°’
                    "published_dt": pub_kst,
                    "published": pub_kst.strftime("%Y-%m-%d %H:%M"),
                    "snippet": snippet,
                }
                stats["added"] += 1

            _sleep()
        except Exception as ex:
            print(f"[WARN] RSS call failed ({track} kw={kw}): {ex}")
            continue

    items = sorted(found.values(), key=lambda a: a["published_dt"], reverse=True)
    return items, stats

# ==========================
# ì›ë¬¸ í™•ì • + í•„í„°
# - ì›ë¬¸ URL í™•ì • ì‹¤íŒ¨(=google ë§í¬ë§Œ ë‚¨ìŒ)ì´ë©´ ë²„ë¦¼ (ìš”êµ¬ì‚¬í•­: google ë§í¬ ê¸ˆì§€)
# ==========================
def finalize_items(items: List[Dict], resolver: PublisherResolver, budget: int) -> Tuple[List[Dict], Dict[str, int]]:
    stats = {"resolved_ok": 0, "resolve_fail_drop": 0, "non_article_url_drop": 0}
    out: List[Dict] = []
    used = 0

    for a in items:
        if used >= budget:
            stats["resolve_fail_drop"] += 1
            continue

        pub_url = resolver.resolve(a["google_link"])
        used += 1

        if not pub_url:
            stats["resolve_fail_drop"] += 1
            continue

        if not is_valid_article_url(pub_url):
            stats["non_article_url_drop"] += 1
            continue

        a["link"] = pub_url
        stats["resolved_ok"] += 1
        out.append(a)

    # ì›ë¬¸ URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    dedup: Dict[str, Dict] = {}
    for a in out:
        sid = _stable_id(a["title"], a["link"])
        dedup[sid] = a

    final = sorted(dedup.values(), key=lambda x: x["published_dt"], reverse=True)
    return final, stats

# ==========================
# Slack ë©”ì‹œì§€
# ==========================
def build_messages(window_label: str,
                   general: List[Dict], nexon: List[Dict],
                   stats: Dict[str, Dict]) -> List[str]:
    header = f"## ğŸ“° ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘\n- ë²”ìœ„: {window_label}\n"
    header += (
        f"- general: feeds={stats['general_rss']['feeds_called']}, seen={stats['general_rss']['entries_seen']}, "
        f"added={stats['general_rss']['added']}, hint_drop={stats['general_rss']['hint_drop']}, "
        f"window_drop={stats['general_rss']['window_drop']}, resolved_ok={stats['general_final']['resolved_ok']}, "
        f"resolve_fail_drop={stats['general_final']['resolve_fail_drop']}, url_drop={stats['general_final']['non_article_url_drop']}\n"
    )
    header += (
        f"- nexon: feeds={stats['nexon_rss']['feeds_called']}, seen={stats['nexon_rss']['entries_seen']}, "
        f"added={stats['nexon_rss']['added']}, hint_drop={stats['nexon_rss']['hint_drop']}, "
        f"window_drop={stats['nexon_rss']['window_drop']}, resolved_ok={stats['nexon_final']['resolved_ok']}, "
        f"resolve_fail_drop={stats['nexon_final']['resolve_fail_drop']}, url_drop={stats['nexon_final']['non_article_url_drop']}\n\n"
    )

    def fmt(a: Dict) -> str:
        sn = f"\n    - {a['snippet']}" if a.get("snippet") else ""
        return f"â–¶ *[{a.get('press','NEWS')}]* <{a['link']}|{a['title']}> ({a['published']}){sn}\n"

    body = "### ğŸŒ ì£¼ìš” ê²Œì„ì—…ê³„ ë‰´ìŠ¤\n"
    if not general:
        body += "- í•´ë‹¹ ë²”ìœ„ì—ì„œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        for a in general[:GENERAL_SEND_LIMIT]:
            body += fmt(a)

    body += "\n---\n### ğŸ¢ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ (Top 5)\n"
    nexon_true = [a for a in nexon if contains_nexon(a["title"], a.get("snippet",""))]
    nexon_sorted = sorted(nexon_true, key=lambda x: (nexon_score(x), x["published_dt"]), reverse=True)

    if not nexon_sorted:
        body += "- í•´ë‹¹ ë²”ìœ„ì—ì„œ ë„¥ìŠ¨ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
    else:
        for a in nexon_sorted[:NEXON_SEND_LIMIT]:
            body += fmt(a)

    full = header + body

    messages: List[str] = []
    chunk = ""
    for line in full.splitlines(True):
        if len(chunk) + len(line) > SLACK_TEXT_LIMIT:
            messages.append(chunk)
            chunk = ""
        chunk += line
    if chunk.strip():
        messages.append(chunk)
    return messages

def send_to_slack(message: str) -> None:
    if not SLACK_WEBHOOK_URL:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    payload = {"text": message}
    resp = requests.post(
        SLACK_WEBHOOK_URL,
        data=json.dumps(payload),
        headers={"Content-Type": "application/json"},
        timeout=15,
    )
    resp.raise_for_status()

# ==========================
# Main
# ==========================
def main() -> None:
    now_kst = datetime.now(KST)
    start_dt, end_dt, window_label = compute_window_kst(now_kst)

    # Google after/beforeëŠ” ë‚ ì§œ ë‹¨ìœ„ë§Œ ì ìš©ë˜ë¯€ë¡œ ë„‰ë„‰íˆ ì¡ê³ ,
    # ì‹¤ì œ ì‹œê°„ í•„í„°ëŠ” RSS published(KST)ë¡œ ì •í™•íˆ ì»·í•œë‹¤.
    after_date = start_dt.date().isoformat()
    before_date = (end_dt.date() + timedelta(days=1)).isoformat()

    general_keywords = PRIMARY_KEYWORDS[:KEYWORD_BATCH_PRIMARY]
    general_raw, stats_general_rss = fetch_track(
        "general", general_keywords, MAX_ENTRIES_PER_FEED,
        build_query_general, after_date, before_date, start_dt, end_dt
    )
    if len(general_raw) < 10:
        general_raw, stats_general_rss = fetch_track(
            "general", PRIMARY_KEYWORDS[:KEYWORD_BATCH_FALLBACK], MAX_ENTRIES_PER_FEED,
            build_query_general, after_date, before_date, start_dt, end_dt
        )

    nexon_raw, stats_nexon_rss = fetch_track(
        "nexon", PRIMARY_KEYWORDS, MAX_ENTRIES_PER_NEXON_FEED,
        build_query_nexon, after_date, before_date, start_dt, end_dt
    )

    resolver = PublisherResolver()
    general_final, stats_general_final = finalize_items(general_raw, resolver, RESOLVE_BUDGET_GENERAL)
    nexon_final, stats_nexon_final = finalize_items(nexon_raw, resolver, RESOLVE_BUDGET_NEXON)

    print(f"[INFO] window: {window_label}")
    print(f"[INFO] general raw={len(general_raw)} final={len(general_final)} stats={stats_general_final}")
    print(f"[INFO] nexon raw={len(nexon_raw)} final={len(nexon_final)} stats={stats_nexon_final}")

    print("[INFO] preview general:")
    for i, a in enumerate(general_final[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. {a['published']} {a['title']} :: {a['link']}")
    print("[INFO] preview nexon:")
    for i, a in enumerate(nexon_final[:PREVIEW_TOP_N], 1):
        print(f"  {i:02d}. {a['published']} {a['title']} :: {a['link']}")

    stats = {
        "general_rss": stats_general_rss,
        "nexon_rss": stats_nexon_rss,
        "general_final": stats_general_final,
        "nexon_final": stats_nexon_final,
    }

    messages = build_messages(window_label, general_final, nexon_final, stats)
    for idx, msg in enumerate(messages, 1):
        send_to_slack(msg)
        print(f"[INFO] sent slack {idx}/{len(messages)}")
        time.sleep(0.15)

if __name__ == "__main__":
    main()
